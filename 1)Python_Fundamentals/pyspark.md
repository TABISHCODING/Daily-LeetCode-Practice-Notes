---
## üß± PHASE 0 ‚Äî Tiny Prerequisites (before PySpark)

**Goal:** Be comfortable with:

* Basic Python (functions, loops, if, imports)
* Very basic SQL (SELECT, WHERE, GROUP BY)

No PySpark questions here, but it makes later parts easy.

---

## üîµ PHASE 1 ‚Äî Big Picture: What is PySpark & Spark?

**Goal:** Understand **what PySpark is**, why it exists, and where it‚Äôs used.

### 1.1 Big Data & Spark

Learn:

* **What is Apache Spark?**

  * A distributed computing engine for processing large datasets.
* **What is a cluster?**

  * Group of machines (nodes) working together.
* **Where does PySpark fit?**

  * PySpark = **Python API for Spark**.

üîó **Questions covered:**

1. What is PySpark?
2. Why do we use PySpark instead of normal Python?
3. Difference between Pandas and PySpark?
4. What is Apache Spark? (very short)
5. What is a cluster in Spark?
6. What are the main advantages of using PySpark?
7. How does PySpark differ from Apache Spark?
8. What are RDDs and how do they differ from DataFrames? (just conceptual)
9. What is the difference between RDD, DataFrame and Dataset? (high level, no depth)

üß† **Core idea:**

* Pandas ‚Üí small data, one machine
* PySpark ‚Üí big data, many machines, with Spark engine

You can remember: **‚ÄúPandas in laptop, PySpark in cluster.‚Äù**

---

## üîµ PHASE 2 ‚Äî SparkSession (Entry Point)

**Goal:** Understand and create `SparkSession`. Without this, nothing works.

### 2.1 What is SparkSession?

* SparkSession is the **main object** in PySpark.
* You use it to:

  * Read files
  * Create DataFrames
  * Run SQL

```python
from pyspark.sql import SparkSession

spark = (
    SparkSession
    .builder
    .appName("demo_app")
    .getOrCreate()
)
```

üîó **Questions covered:**
10. What is SparkSession?
11. Why do we need SparkSession?
12. How do you create a SparkSession?
13. Describe different ways to read data into PySpark. (You‚Äôll expand in next phases.)

---

## üîµ PHASE 3 ‚Äî DataFrames: Your Main Weapon

**Goal:** Be fully comfortable with PySpark **DataFrames**.

### 3.1 What is a DataFrame?

Learn:

* DataFrame = **table-like structure** (rows & columns), distributed.
* Similar to SQL table or Pandas DataFrame, but handled by Spark.

üîó **Questions:**
14. What is a DataFrame in PySpark?
15. How is a DataFrame different from a SQL table?

### 3.2 Reading a CSV

Use:

```python
df = spark.read.csv(
    "data.csv",
    header=True,
    inferSchema=True
)
```

Understand:

* `header=True` ‚Üí first row is column names
* `inferSchema=True` ‚Üí Spark auto-detects types

üîó **Questions:**
16. How do you read a CSV file in PySpark?
17. What do `header=True` and `inferSchema=True` mean?

### 3.3 Inspecting Data

Use:

```python
df.show()          # first 20 rows
df.printSchema()   # structure and data types
```

üîó **Questions:**
18. How to view data in a DataFrame? (`show()`)
19. How to check schema of a DataFrame? (`printSchema()`)

### 3.4 Selecting & Filtering

```python
df.select("name", "age").show()
df.filter(df.age > 25).show()
```

üîó **Questions:**
20. How to select columns in PySpark?
21. How to filter rows in PySpark?
22. What methods can be used for filtering in PySpark DataFrames?

### 3.5 Grouping, WithColumn, Missing Data & Misc

```python
from pyspark.sql import functions as F

df.groupBy("department").agg(F.avg("salary")).show()
df2 = df.withColumn("bonus", df.salary * 0.1)
df_clean = df.dropna()
df_filled = df.fillna({"age": 0})
df_sorted = df.orderBy("salary")
df_distinct = df.select("department").distinct()
```

üîó **Questions:**
23. Explain the use of `groupBy` and `agg` functions in PySpark.
24. What is the use of `withColumn` function?
25. How do you handle missing data in PySpark?
26. What is the difference between `union` and `unionByName` in PySpark? (Just know: `unionByName` matches columns by name.)
27. How can you sort or order data in a DataFrame?
28. Describe the `distinct` function and its use cases.

### 3.6 UDFs & Pandas ‚áÑ PySpark Conversion (Just Basics)

Concept only:

* UDF = **User Defined Function**, when built-in functions aren‚Äôt enough.
* Conversion:

```python
pandas_df = df.toPandas()
spark_df = spark.createDataFrame(pandas_df)
```

üîó **Questions:**
29. What is a UDF in PySpark, and how do you use it? (High-level understanding)
30. How can you convert a Pandas DataFrame to a PySpark DataFrame and vice versa?
24. What are the differences between PySpark and pandas? (Reinforces Pandas vs PySpark)

---

## üîµ PHASE 4 ‚Äî Transformations vs Actions

**Goal:** Understand **lazy evaluation** and how Spark actually runs jobs.

### 4.1 Transformations

* Don‚Äôt run immediately.
* Create a **plan**.
* Examples:

```python
df2 = df.select("name", "age")
df3 = df2.filter(df2.age > 25)
df4 = df3.withColumn("double_age", df3.age * 2)
```

### 4.2 Actions

* Trigger execution.
* Examples:

```python
df4.show()
df4.count()
rows = df4.collect()
```

### 4.3 Lazy Evaluation

* Spark waits until an **action** is called.
* Optimizes the full plan at once.

üîó **Questions:**
31. What is a transformation in PySpark?
32. What is an action in PySpark?
33. Give examples of transformations.
34. Give examples of actions.
35. What is lazy evaluation in PySpark?
36. Why does Spark use lazy evaluation?

---

## üîµ PHASE 5 ‚Äî Basic Operations & Aggregations

**Goal:** Do most common business tasks.

### 5.1 Adding New Column

```python
df = df.withColumn("bonus", df.salary * 0.1)
```

### 5.2 Grouping & Aggregations

```python
from pyspark.sql import functions as F

df.groupBy("department").agg(
    F.avg("salary").alias("avg_salary"),
    F.max("salary").alias("max_salary"),
    F.sum("salary").alias("total_salary")
).show()
```

üîó **Questions:**
37. How do you add a new column using `withColumn`?
38. How do you calculate bonus or percentage column?
39. How do you group data in PySpark? (groupBy + agg)
40. How do you calculate average, sum, max in a group?

---

## üîµ PHASE 6 ‚Äî Joins (Very Important)

**Goal:** Understand and code joins between DataFrames.

Assume:

```python
emp(id, name, dept_id, salary)
dept(id, dept_name)
```

### 6.1 Inner Join

```python
emp.join(dept, emp.dept_id == dept.id, "inner")
```

### 6.2 Left / Right / Full

```python
emp.join(dept, emp.dept_id == dept.id, "left")
emp.join(dept, emp.dept_id == dept.id, "right")
emp.join(dept, emp.dept_id == dept.id, "full")
```

üîó **Questions:**
41. What is a join in PySpark?
42. What are the types of joins in PySpark?
43. How do you perform a join in PySpark?
44. What is a left join? Explain with example.

---

## üîµ PHASE 7 ‚Äî Handling NULL Values

**Goal:** Not fail when data is dirty (common in real life).

### 7.1 Dropping NULLs

```python
df2 = df.dropna()                  # any null
# or specific columns
df2 = df.dropna(subset=["age", "salary"])
```

### 7.2 Filling NULLs

```python
df2 = df.fillna({"age": 0, "salary": 0})
```

### 7.3 Why Important?

* Avoid wrong aggregations
* Avoid errors in joins, calculations, etc.

üîó **Questions:**
45. How do you drop rows with NULL values? (`dropna`)
46. How do you fill NULL values? (`fillna`)
47. Why is NULL handling important in data processing?

---

## üîµ PHASE 8 ‚Äî File Formats & Schema

**Goal:** Know basic file formats & why Parquet is loved.

### 8.1 Supported Formats

PySpark commonly works with:

* CSV
* JSON
* Parquet
* ORC
* (sometimes Avro)

### 8.2 Why Parquet?

* Columnar
* Compressed
* Reads only needed columns

### 8.3 JSON

* Good for nested/hierarchical data.

### 8.4 Schema

* Schema = structure of DataFrame (columns, types).
* Important for consistency, performance, compatibility.

üîó **Questions:**
48. What file formats does PySpark support?
49. Why is Parquet faster than CSV?
50. What is JSON used for?
51. Which format is best for big data use cases?
52. How do you work with JSON, Parquet, or Avro in PySpark? (concept-level)
53. What is a DataFrame schema and why is it important?

---

## üîµ PHASE 9 ‚Äî SQL in PySpark

**Goal:** Run SQL on top of DataFrames.

### 9.1 Creating a Temp View

```python
df.createOrReplaceTempView("emp")
```

### 9.2 Running SQL

```python
result = spark.sql("SELECT name, age FROM emp WHERE age > 30")
result.show()
```

You can:

* Filter with `WHERE`
* Group with `GROUP BY`
* Aggregate with `AVG`, `SUM`, etc.

üîó **Questions:**
54. How do you create a temporary SQL table in PySpark?
55. How do you run SQL queries using `spark.sql`?
56. How do you filter data using SQL queries in PySpark?
57. How do you select columns using SQL in PySpark?

---

## üîµ PHASE 10 ‚Äî Mini ETL Task (Must Be Confident)

**Goal:** Be able to **speak through a tiny ETL pipeline**.

Assume DataFrame:

`df(id, name, age, salary)`

### 10.1 Filter employees age > 30

```python
df2 = df.filter(df.age > 30)
```

### 10.2 Add bonus column

```python
df3 = df2.withColumn("bonus", df2.salary * 0.1)
```

### 10.3 Group by age, get average salary

```python
from pyspark.sql import functions as F

df4 = df3.groupBy("age").agg(F.avg("salary").alias("avg_salary"))
df4.show()
```

### 10.4 Explain ETL Steps (In Words)

* **E (Extract)**: Data read from CSV/Parquet into DataFrame.
* **T (Transform)**:

  * Filter rows where age > 30
  * Add a new column `bonus`
  * Group by age and compute avg salary
* **L (Load)**:

  * Could be written back to a file or used in a report.

üîó **Questions:**
58. How do you filter employees whose age > 30?
59. How do you add a bonus column = 10% salary?
60. How do you group by age and find average salary?
61. Explain the ETL steps you performed.

---

## üîµ PHASE 11 ‚Äî Practical & HR Answers

**Goal:** Have confident, honest answers when they ask about your PySpark experience.

### 11.1 ‚ÄúHave you done any PySpark project?‚Äù

You:

> ‚ÄúI don‚Äôt have an end-to-end PySpark project yet, but I‚Äôve used PySpark to practice DataFrames, joins, aggregations, simple ETL operations, and SQL queries. My main projects are in Flask + Python + SQL, and I‚Äôm actively learning PySpark fundamentals.‚Äù

### 11.2 ‚ÄúWhat parts of PySpark have you practiced?‚Äù

You:

> ‚ÄúI‚Äôve practiced reading CSV data into DataFrames, selecting and filtering columns, using withColumn, groupBy + agg, handling NULL values, performing joins, using Parquet files, and running SQL queries through `spark.sql`.‚Äù

### 11.3 ‚ÄúAre you comfortable learning PySpark on the job?‚Äù

You:

> ‚ÄúYes. Since PySpark is based on Python and similar to Pandas DataFrame operations, I‚Äôm comfortable learning and extending it on the job.‚Äù

üîó **Questions:**
62. Have you done any PySpark project?
63. What parts of PySpark have you practiced so far?
64. Are you comfortable learning PySpark on the job?

---

## ‚úÖ How to Use This Roadmap (Very Practical)

If you want to **study using this roadmap**, do this:

1. **Phase 1 ‚Üí 3**:

   * Understand what PySpark is.
   * Set up SparkSession.
   * Load a CSV and play with `show()`, `printSchema()`, `select`, `filter`.

2. **Phase 4 ‚Üí 7**:

   * Practice transformations vs actions.
   * Add columns, groupBy, joins, handle NULLs.

3. **Phase 8 ‚Üí 9**:

   * Learn file formats and SQL with `createOrReplaceTempView` and `spark.sql`.

4. **Phase 10**:

   * Implement the small ETL task on sample data.
   * Be able to explain it **verbally** like an interviewer is asking.

5. **Phase 11**:

   * Memorize your HR/safe answers.

If you want next, I can:

* Turn **each phase** into a **lesson with code + short answers to the exact questions**, or
* Build a **2‚Äì3 day PySpark study plan** using this roadmap.


Alright, welcome to **Phase 1** üòÑ
We‚Äôll do exactly this:

> **Learn concept ‚Üí See tiny code (where useful) ‚Üí See which interview questions it answers ‚Üí Get a few small practice Qs**

Only **basic level**, no deep architecture, no tuning, no cluster admin stuff.

---

# üîµ PHASE 1 ‚Äî Big Picture: What is PySpark & Spark?

üéØ **Goal for this phase:**
By the end, you should be able to confidently explain:

* What is **Spark**?
* What is **PySpark**?
* Why do we need it if we already have Python & Pandas?
* What is a **cluster**?
* What are **RDD, DataFrame, Dataset** (just at bird‚Äôs-eye level)?

This is 100% theory + mental model, very light code.

---

## 1Ô∏è‚É£ Step 1 ‚Äî What problem does PySpark solve?

### üß† Concept

First, imagine:

* You have a **small CSV file** with 10,000 rows ‚Üí Pandas is OK.
* Now imagine a **huge log file** with **200 GB** of data.

Problems with normal Python / Pandas:

* File might not even fit in RAM.
* Processing will be very slow.
* Only **one CPU** is used.

So we need something that can:

* Split the data across **many machines**.
* Use **many CPUs** in parallel.
* Still give us a nice **table-like API** (like DataFrame).

‚û°Ô∏è That ‚Äúsomething‚Äù is **Apache Spark**.
‚û°Ô∏è And **PySpark** is how we talk to Spark using **Python**.

---

## 2Ô∏è‚É£ Step 2 ‚Äî What is Apache Spark?

### üß† Concept

> **Apache Spark** is a **distributed computing engine** for processing large-scale data.

Key words (you can say this in interview):

* **Distributed** ‚Üí runs on multiple machines.
* **In-memory** ‚Üí keeps data in RAM when possible (faster than disk).
* **General-purpose** ‚Üí supports batch jobs, SQL, streaming, ML, etc.

You don‚Äôt need deep architecture. At apprentice level, this is enough:

> ‚ÄúSpark helps us process big data quickly by distributing it across multiple machines.‚Äù

---

## 3Ô∏è‚É£ Step 3 ‚Äî What is a cluster? (Beginner meaning)

### üß† Concept

> A **cluster** = a **group of machines** working together as if they are one big machine.

Very simple:

* Each machine ‚Üí called a **node**.
* One special node controls others (driver/coordinator).
* Other nodes do the actual work (workers).

You don‚Äôt need driver/executor details yet. Just say:

> ‚ÄúSpark usually runs on a cluster, which is a group of machines that share the work.‚Äù

---

## 4Ô∏è‚É£ Step 4 ‚Äî Where does PySpark fit?

Now we combine everything.

* **Spark** = big data engine (written in Scala/Java).
* **PySpark** = **Python API** that lets you control Spark from Python code.

Analogy:

* Spark = car engine.
* PySpark = steering wheel written in Python.

So when you run:

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("demo").getOrCreate()
```

You are actually:

* Using **Python** to tell **Spark** what to do.
* Spark internally distributes the work on the cluster.

---

## 5Ô∏è‚É£ Step 5 ‚Äî Pandas vs PySpark vs ‚Äúnormal Python‚Äù

### üß† Concept

You can think like this:

| Tool          | Runs on                 | Best for                           |
| ------------- | ----------------------- | ---------------------------------- |
| Normal Python | Single CPU              | Simple scripts, logic              |
| Pandas        | Single machine (RAM)    | Small/medium data (MB‚ÄìGB)          |
| PySpark       | Many machines (cluster) | Big data (tens/hundreds of GB, TB) |

So:

* If **data size < RAM of your laptop** ‚Üí Pandas is fine.
* If **data is very large or you want distributed processing** ‚Üí PySpark.

üëâ This gives you a **perfect interview line**:

> ‚ÄúPandas is good for small data on one machine, PySpark is for big data on a cluster.‚Äù

---

## 6Ô∏è‚É£ Step 6 ‚Äî What are RDD, DataFrame, Dataset? (Only high-level)

We don‚Äôt go deep. You just need **conceptual understanding**.

### üß† Concept

* **RDD (Resilient Distributed Dataset)**

  * Low-level data structure in Spark.
  * Like a distributed list of objects.
  * More control, but more code.
  * Not usually needed in beginner PySpark for interviews.

* **DataFrame**

  * Table with rows and columns (like SQL or Pandas).
  * Higher-level, optimized, easy to use.
  * This is your **main focus**.

* **Dataset** (mainly in Scala/Java, not used much in Python)

  * Like typed DataFrames (with compile-time type safety).
  * **Not important** for you now as a Python + PySpark candidate.

Best line for interview:

> ‚ÄúIn PySpark, we mostly use DataFrames. RDDs are the lower-level API, and Datasets are more common in Scala/Java, not in Python.‚Äù

---

## 7Ô∏è‚É£ Tiny Code Glimpse (Just to Visualize)

Even though this is conceptual, let‚Äôs see a **very small code snippet** to connect theory:

```python
from pyspark.sql import SparkSession

# 1. Create SparkSession (entry point)
spark = SparkSession.builder.appName("phase1_demo").getOrCreate()

# 2. Read a small CSV into a DataFrame
df = spark.read.csv("employees.csv", header=True, inferSchema=True)

# 3. Show some rows
df.show()

# 4. Print schema
df.printSchema()
```

At this stage you don‚Äôt need to run it, just **understand**:

* We‚Äôre using **PySpark (Python)** to control **Spark**.
* Spark can run on a **cluster**, even if for local learning it runs on your machine.
* Data is in a **DataFrame**, not RDD.

---

## 8Ô∏è‚É£ Which interview questions does Phase 1 cover?

From our merged list, Phase 1 helps you answer:

1. What is PySpark?
2. Why do we use PySpark instead of normal Python?
3. Difference between Pandas and PySpark?
4. What is Apache Spark? (very short)
5. What is a cluster in Spark?
6. What are the main advantages of using PySpark?
7. How does PySpark differ from Apache Spark?
8. What are RDDs and how do they differ from DataFrames? (just conceptual)
9. What is the difference between RDD, DataFrame and Dataset? (high level)

If you can confidently explain everything we just discussed, you already have **Phase 1 fully covered.**

---

## 9Ô∏è‚É£ Small Practice Section (for you to think/answer)

These are NOT advanced coding questions ‚Äì just to test your understanding.

### üß© Practice Q1 (Concept)

**Q:** Your CSV file is 50 MB and runs fine with Pandas. Should you immediately move to PySpark for it? Why / why not?

*(Think: do you really need a cluster for that?)*

---

### üß© Practice Q2 (Concept)

**Q:** Your company has 2 TB of log data stored on a distributed file system. Is it better to use:

* a) normal Python loops
* b) Pandas
* c) PySpark DataFrames

Explain in one or two lines.

---

### üß© Practice Q3 (Interview simulation)

**Q (Interviewer):** ‚ÄúIn simple words, what is a cluster?‚Äù
Try to answer in **one sentence**.

---

### üß© Practice Q4 (Difference question)

**Q:** Say one difference between:

* RDD vs DataFrame
* DataFrame vs Dataset

(Only high-level, 1 line each.)

---

### üß© Practice Q5 (Tiny code reading)

Look at this code:

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("check").getOrCreate()
df = spark.read.csv("users.csv", header=True, inferSchema=True)
df.show()
```

Answer:

1. Which part is PySpark?
2. Which object represents Spark?
3. What type of thing is `df`?

---
**Phase 1 interview answers + all practice question answers**, written in the **exact style you must speak in interviews**.
---

# ‚úÖ **PHASE 1 ‚Äî Interview Questions & Perfect Answers**

Below are the answers to the 9 questions covered in Phase 1.

---

## **1Ô∏è‚É£ What is PySpark?**

PySpark is the Python API for Apache Spark. It lets us process large datasets in a distributed way using Python.

---

## **2Ô∏è‚É£ Why do we use PySpark instead of normal Python?**

Normal Python runs on a single machine and processes data slowly, while PySpark distributes data across multiple machines, making it much faster for big data.

---

## **3Ô∏è‚É£ Difference between Pandas and PySpark?**

* Pandas ‚Üí small data, single machine
* PySpark ‚Üí big data, many machines (distributed processing)

---

## **4Ô∏è‚É£ What is Apache Spark? (very short)**

Apache Spark is a distributed computing engine used to process large datasets quickly.

---

## **5Ô∏è‚É£ What is a cluster in Spark?**

A cluster is a group of connected machines that work together to process data.

---

## **6Ô∏è‚É£ What are the main advantages of using PySpark?**

* Handles large data efficiently
* Uses multiple machines (distributed)
* Very fast due to in-memory execution
* Easy DataFrame API like Pandas
* Allows SQL, streaming, ML, etc.

---

## **7Ô∏è‚É£ How does PySpark differ from Apache Spark?**

Apache Spark is the engine; PySpark is the Python interface used to work with Spark.

---

## **8Ô∏è‚É£ What are RDDs and how do they differ from DataFrames? (simple)**

RDDs are low-level distributed collections without schema; DataFrames are high-level tables with named columns.

---

## **9Ô∏è‚É£ Difference between RDD, DataFrame, and Dataset? (very high-level)**

* RDD ‚Üí low-level, unstructured
* DataFrame ‚Üí high-level, structured
* Dataset ‚Üí strongly typed DataFrame (not used in Python)

---

# üß† **Phase 1 is complete ‚Äî Now the Practice Question Answers**

---

# üß© Practice Q1

**Q:** A CSV file is 50 MB and runs fine in Pandas. Should you move to PySpark?

### ‚úÖ **Answer:**

**No.**
Pandas is enough for 50 MB because it easily fits into memory. You only need PySpark when the data becomes very large (GBs or TBs).

---

# üß© Practice Q2

**Q:** Your company has 2 TB logs. Which option is best?

a) Python loops
b) Pandas
c) PySpark DataFrames

### ‚úÖ **Answer:**

**c) PySpark DataFrames**
Because 2 TB is too large for one machine; PySpark can distribute the data across a cluster.

---

# üß© Practice Q3

**Q:** ‚ÄúIn simple words, what is a cluster?‚Äù

### ‚úÖ **Answer:**

A cluster is a group of machines working together to process data faster.

---

# üß© Practice Q4

**Q:** One-line differences:

### **RDD vs DataFrame**

RDD ‚Üí low-level, no schema
DataFrame ‚Üí table with columns, optimized

### **DataFrame vs Dataset**

DataFrame ‚Üí dynamic typing
Dataset ‚Üí strong typing (not used in Python)

---

# üß© Practice Q5

### Code:

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("check").getOrCreate()
df = spark.read.csv("users.csv", header=True, inferSchema=True)
df.show()
```

### Q1: Which part is PySpark?

`from pyspark.sql import SparkSession` and all code using `spark` and `df` are PySpark.

### Q2: Which object represents Spark?

`spark` is the SparkSession object that represents Spark.

### Q3: What type of thing is df?

`df` is a **PySpark DataFrame**.

---

# üéâ **Phase 1 Completed 100%**

You are ready to move to Phase 2.

Say: **‚ÄúGo to Phase 2‚Äù**
and I will start:

Absolutely ‚Äî here is **PHASE 2**, explained *exactly like Phase 1*:

‚úî Learn concept
‚úî See small code
‚úî Know which interview questions this covers
‚úî Do practice questions
‚úî Do small coding tasks

Beginner-friendly, no advanced internal architecture.

---

# üîµ **PHASE 2 ‚Äî SparkSession (Entry Point of PySpark)**

### üéØ Phase Goal:

By the end of Phase 2, you should be able to answer:

* What is SparkSession?
* Why is it needed?
* How to create it?
* How to read data using SparkSession?

This phase is *very important* because every PySpark program starts with SparkSession.

---

# 2.1 **What is SparkSession?**

### üß† Concept (beginner level)

> **SparkSession is the main entry point to PySpark.**
> It is like the ‚Äúcontroller‚Äù or ‚Äúbrain‚Äù of your PySpark program.

You need it for:

* Reading files (`read`)
* Creating DataFrames
* Running SQL queries (`spark.sql`)
* Configuring the app (name, settings)
* Connecting to the cluster (internally)

Think of it as:

üü¶ **PySpark ‚Üí SparkSession ‚Üí Everything happens through this**

Just like:

* Flask ‚Üí `app`
* PySpark ‚Üí `spark`

---

# 2.2 **Why do we need SparkSession?**

### üß† Simple explanation:

Without SparkSession:

‚ùå You cannot read files.
‚ùå You cannot create DataFrames.
‚ùå You cannot run SQL queries.
‚ùå PySpark cannot talk to the Spark engine.

So the rule is:

> **No SparkSession = No PySpark program.**

---

# 2.3 **How to Create SparkSession? (Must memorize)**

This is the **exact** code the interviewer expects you to know:

```python
from pyspark.sql import SparkSession

spark = (
    SparkSession
    .builder
    .appName("demo_app")
    .getOrCreate()
)
```

### üîç Break it down:

* `SparkSession.builder` ‚Üí start building a SparkSession
* `.appName("demo_app")` ‚Üí name your app
* `.getOrCreate()` ‚Üí create new one or reuse existing

### üß† Interview trick:

If they ask:

> ‚ÄúWhat does getOrCreate() do?‚Äù

You say:

> ‚ÄúIt creates a SparkSession if it doesn‚Äôt exist, otherwise returns the existing one.‚Äù

---

# 2.4 **How do we read data using SparkSession?**

SparkSession gives you `spark.read`

```python
df = spark.read.csv("data.csv", header=True, inferSchema=True)
```

Later, you will use:

* `spark.read.json()`
* `spark.read.parquet()`
* `spark.read.format()`

But for now, CSV is enough.

---

# üîó **Interview Questions Covered in Phase 2**

10. What is SparkSession?
11. Why do we need SparkSession?
12. How do you create a SparkSession?
13. Describe different ways to read data into PySpark.

And an extra one they sometimes ask:

‚ùì **What does getOrCreate() do?**

‚úî You already learned that.

---

# üìò **Mini Code Example (Beginner-friendly)**

Just to visualize everything:

```python
from pyspark.sql import SparkSession

# 1. Create SparkSession
spark = SparkSession.builder.appName("example").getOrCreate()

# 2. Read CSV file
df = spark.read.csv("employees.csv", header=True, inferSchema=True)

# 3. Show first rows
df.show()
```

That‚Äôs it. No complexity.

---

# üß© PRACTICE QUESTIONS (For Understanding)

### **Q1:**

What is SparkSession in PySpark?
(Give a 1-line interview answer.)

---

### **Q2:**

Why do we write `.getOrCreate()` at the end of SparkSession?

---

### **Q3:**

Which object is responsible for reading files in PySpark?

* a) pandas
* b) spark
* c) df
* d) SQLContext

---

### **Q4:**

Write the code to create a SparkSession named ‚Äútest_app‚Äù.

*(Write exact 3 lines.)*

---

### **Q5:**

Why do we need SparkSession before reading a CSV file?

---

# üß™ PRACTICE CODING TASK (Very Simple)

**Task:**
Write PySpark code to:

1. Create a SparkSession named ‚Äúpractice_session‚Äù
2. Read a file called `students.csv`
3. Print first 20 rows
4. Print the schema

---
Perfect ‚Äî here are the **Phase 2 interview answers + practice question answers**, written exactly in interview-perfect format.

---

# ‚úÖ **PHASE 2 ‚Äî Interview Questions & Perfect Answers**

These are the answers to the 4 official interview questions covered in Phase 2.

---

## **10. What is SparkSession?**

SparkSession is the entry point to any PySpark application.
We use it to create DataFrames, read files, and run SQL queries.

---

## **11. Why do we need SparkSession?**

Because without SparkSession, PySpark cannot connect to the Spark engine.
It is required for reading data, creating DataFrames, and executing Spark operations.

---

## **12. How do you create a SparkSession?**

This is the exact syntax:

```python
from pyspark.sql import SparkSession

spark = (
    SparkSession
    .builder
    .appName("demo_app")
    .getOrCreate()
)
```

---

## **13. Describe different ways to read data into PySpark.**

We use `spark.read` to load data:

* `spark.read.csv("file.csv")`
* `spark.read.json("file.json")`
* `spark.read.parquet("file.parquet")`
* `spark.read.format("csv").load(path)`

For beginners, CSV is enough.

---

## ‚≠ê Bonus Mini-Question (often asked)

**Q: What does getOrCreate() do?**
It creates a new SparkSession if one doesn't exist; otherwise it returns the existing one.

---

# üß© **Practice Question Answers**

These test your understanding and help you speak confidently.

---

## **Practice Q1:**

**Q:** What is SparkSession in PySpark?
**A:**
SparkSession is the main object in PySpark that allows us to read data, create DataFrames, and run SQL queries.

---

## **Practice Q2:**

**Q:** Why do we use `.getOrCreate()`?
**A:**
Because it either creates a new SparkSession or returns the existing one. This prevents duplicate sessions.

---

## **Practice Q3:**

**Q:** Which object reads files in PySpark?

Options:
a) pandas
b) spark
c) df
d) SQLContext

**Correct Answer:** **b) spark**
Because `spark.read` is used to load data into DataFrames.

---

## **Practice Q4:**

**Q:** Write the code to create a SparkSession named ‚Äútest_app‚Äù.

### ‚úÖ Answer:

```python
from pyspark.sql import SparkSession

spark = (
    SparkSession
    .builder
    .appName("test_app")
    .getOrCreate()
)
```

---

## **Practice Q5:**

**Q:** Why do we need SparkSession before reading a CSV file?
**A:**
Because SparkSession gives us the `spark.read` interface.
Without SparkSession, PySpark cannot load files or create DataFrames.

---

# üß™ Practice Coding Task ‚Äî SOLUTION

**Task:** Create SparkSession ‚Üí Read CSV ‚Üí show rows ‚Üí print schema

### ‚úÖ Solution:

```python
from pyspark.sql import SparkSession

# 1. Create SparkSession
spark = (
    SparkSession
    .builder
    .appName("practice_session")
    .getOrCreate()
)

# 2. Read CSV
df = spark.read.csv("students.csv", header=True, inferSchema=True)

# 3. Show first rows
df.show()

# 4. Print schema
df.printSchema()
```

---

# üéâ Phase 2 is 100% complete.
Perfect ‚Äî here is **PHASE 3 explained exactly like Phase 1 & Phase 2**, including:

‚úî Concept
‚úî Mini-code
‚úî Interview answers (for ALL 17 questions in this phase)
‚úî Practice questions
‚úî Small coding tasks

No advanced topics, only beginner-level DataFrame skills.

---

# üîµ **PHASE 3 ‚Äî DataFrames: Your Main Weapon**

DataFrames are **the MOST important** part of PySpark for your apprenticeship round.
If you master Phase 3 + joins + SQL ‚Üí you already look experienced.

Let‚Äôs walk step-by-step.

---

# ‚úÖ **3.1 What is a DataFrame?**

### üß† Concept

A **PySpark DataFrame** is:

* A **distributed table** with rows & columns
* Similar to:

  * SQL table
  * Pandas DataFrame
* But stored & processed **in parallel on multiple machines**

You can think:

> ‚ÄúDataFrame = a table spread across many computers.‚Äù

### üí¨ Interview Questions & Answers

### **Q14. What is a DataFrame in PySpark?**

A DataFrame is a distributed collection of data organized into named columns, similar to a SQL table or Pandas DataFrame.

---

### **Q15. How is a DataFrame different from a SQL table?**

A DataFrame is an in-memory distributed table processed by Spark, whereas a SQL table is stored in a database.

---

# ‚úÖ **3.2 Reading a CSV File**

### üß† Concept

Syntax:

```python
df = spark.read.csv(
    "data.csv",
    header=True,
    inferSchema=True
)
```

### Important parameters:

* **header=True**
  Means first row of the CSV contains column names.

* **inferSchema=True**
  Spark automatically detects correct data types
  (e.g., age ‚Üí integer, salary ‚Üí double)

### üí¨ Interview Questions & Answers

### **Q16. How do you read a CSV file in PySpark?**

```python
df = spark.read.csv("file.csv", header=True, inferSchema=True)
```

---

### **Q17. What do header=True and inferSchema=True mean?**

* `header=True` ‚Üí first row contains column names
* `inferSchema=True` ‚Üí Spark automatically detects data types

---

# ‚úÖ **3.3 Inspecting Data**

### üß† Concept

These two commands are mandatory:

```python
df.show()        # prints first 20 rows
df.printSchema() # prints column names & data types
```

### üí¨ Interview Questions & Answers

### **Q18. How to view data in a DataFrame?**

Use:

```python
df.show()
```

---

### **Q19. How to check schema of a DataFrame?**

Use:

```python
df.printSchema()
```

---

# ‚úÖ **3.4 Selecting & Filtering Columns**

### üß† Concept

```python
df.select("name", "age").show()
df.filter(df.age > 25).show()
```

### üí¨ Interview Questions & Answers

### **Q20. How to select columns in PySpark?**

```python
df.select("column1", "column2")
```

---

### **Q21. How to filter rows in PySpark?**

```python
df.filter(df.age > 25)
```

---

### **Q22. What methods can be used for filtering in PySpark?**

* `filter()`
* `where()`

---

# ‚úÖ **3.5 Grouping, WithColumn, Missing Data & Misc Operations**

### üß† Concept + Code

```python
from pyspark.sql import functions as F

# Grouping
df.groupBy("department").agg(F.avg("salary")).show()

# Add a new column
df2 = df.withColumn("bonus", df.salary * 0.1)

# Drop nulls
df_clean = df.dropna()

# Fill nulls
df_filled = df.fillna({"age": 0})

# Sort
df_sorted = df.orderBy("salary")

# Unique values
df_distinct = df.select("department").distinct()
```

---

### üí¨ Interview Questions & Answers

### **Q23. Explain groupBy & agg functions.**

They are used to group rows by a column and apply aggregations such as avg, sum, max, min, count.

---

### **Q24. What is the use of withColumn()?**

It is used to add a new column or update an existing column.

---

### **Q25. How do you handle missing data in PySpark?**

* Remove missing values ‚Üí `dropna()`
* Replace missing values ‚Üí `fillna()`

---

### **Q26. Difference between union and unionByName?**

`union()` ‚Üí matches columns by position
`unionByName()` ‚Üí matches columns by name

---

### **Q27. How can you sort or order data in a DataFrame?**

```python
df.orderBy("salary")
```

---

### **Q28. What does distinct() do?**

It returns unique values from a column or DataFrame.

---

# ‚úÖ **3.6 UDFs & Pandas ‚áÑ PySpark Conversion**

### üß† Concept

A **UDF (User Defined Function)** allows custom Python functions to run on DataFrame columns.

*(Beginner level: just know what it is. No need to write UDF code yet.)*

### **Conversion**

```python
pandas_df = df.toPandas()
spark_df = spark.createDataFrame(pandas_df)
```

---

### üí¨ Interview Questions & Answers

### **Q29. What is a UDF in PySpark?**

A UDF is a User Defined Function that allows you to apply custom Python logic to PySpark columns.

---

### **Q30. How can you convert Pandas DataFrame to PySpark and vice versa?**

```python
# PySpark ‚Üí Pandas
pandas_df = df.toPandas()

# Pandas ‚Üí PySpark
spark_df = spark.createDataFrame(pandas_df)
```

---

### **Extra: What are differences between Pandas & PySpark?**

* Pandas ‚Üí single machine
* PySpark ‚Üí distributed
* Pandas ‚Üí eager
* PySpark ‚Üí lazy

---

# üéâ **PHASE 3 ‚Äî ALL INTERVIEW ANSWERS DONE**

Now let‚Äôs test your understanding.

---

# üß© PHASE 3 ‚Äî PRACTICE QUESTIONS (Answer These)

### **Q1:** What is a DataFrame in PySpark?

---

### **Q2:** What do header=True and inferSchema=True mean?

---

### **Q3:** Write a command to select only ‚Äúname‚Äù and ‚Äúsalary‚Äù columns.

---

### **Q4:** Write a command to filter employees whose age > 40.

---

### **Q5:** How do you add a new column ‚Äúbonus = salary * 0.05‚Äù?

---

### **Q6:** How do you drop rows with NULLs?

---

### **Q7:** What is the difference between union and unionByName?

---

### **Q8:** How do you get unique department names from a DataFrame?

---

# üß™ Coding Practice Task

Use PySpark to:

1. Read `employees.csv`
2. Select only: name, age, department
3. Add a column: `tax = salary * 0.1`
4. Filter: age > 25
5. Group by department and show avg salary

---
Absolutely ‚Äî here are the **perfect, beginner-friendly, interview-ready answers for all Phase 3 questions**, PLUS the **practice question answers** and the **coding task solution**.

I will follow the exact format used in Phase 1 and Phase 2.

---

# ‚úÖ **PHASE 3 ‚Äî Interview Questions & Perfect Answers**

Below are the answers to **all 17 DataFrame questions** from Phase 3.

---

# üîµ **3.1 ‚Äî What is a DataFrame?**

### **Q14. What is a DataFrame in PySpark?**

A DataFrame is a distributed table with rows and columns, similar to a SQL table or Pandas DataFrame, but processed in parallel by Spark.

---

### **Q15. How is a DataFrame different from a SQL table?**

A DataFrame is stored in memory and processed by Spark across multiple machines, while a SQL table is stored in a database system.

---

# üîµ **3.2 ‚Äî Reading a CSV**

### **Q16. How do you read a CSV file in PySpark?**

```python
df = spark.read.csv("file.csv", header=True, inferSchema=True)
```

---

### **Q17. What do header=True and inferSchema=True mean?**

* `header=True` ‚Üí The first row contains column names.
* `inferSchema=True` ‚Üí Spark automatically detects correct data types.

---

# üîµ **3.3 ‚Äî Inspecting Data**

### **Q18. How do you view data in a DataFrame?**

Use:

```python
df.show()
```

---

### **Q19. How do you check the schema of a DataFrame?**

Use:

```python
df.printSchema()
```

---

# üîµ **3.4 ‚Äî Selecting & Filtering**

### **Q20. How do you select columns in PySpark?**

```python
df.select("name", "age")
```

---

### **Q21. How do you filter rows in PySpark?**

```python
df.filter(df.age > 25)
```

---

### **Q22. What methods can be used for filtering?**

* `filter()`
* `where()`

Both work the same.

---

# üîµ **3.5 ‚Äî Grouping, WithColumn, Missing Data & Misc**

### **Q23. Explain the use of groupBy and agg.**

`groupBy` groups rows by a column, and `agg` applies aggregations like avg, sum, max, min.

---

### **Q24. What is the use of withColumn()?**

Used to create a new column or update an existing column.

---

### **Q25. How do you handle missing data in PySpark?**

* `dropna()` ‚Üí remove rows with NULLs
* `fillna()` ‚Üí replace NULL values with defaults

---

### **Q26. Difference between union and unionByName?**

* `union()` ‚Üí matches columns by position
* `unionByName()` ‚Üí matches columns by column name

---

### **Q27. How can you sort or order data in a DataFrame?**

```python
df.orderBy("salary")
```

---

### **Q28. What does distinct() do?**

Returns unique values of a column or unique rows of the DataFrame.

---

# üîµ **3.6 ‚Äî UDFs & Pandas Conversion**

### **Q29. What is a UDF in PySpark?**

A UDF (User Defined Function) lets you apply custom Python logic on DataFrame columns.

---

### **Q30. How can you convert a Pandas DataFrame to PySpark and vice versa?**

```python
# PySpark ‚Üí Pandas
pandas_df = df.toPandas()

# Pandas ‚Üí PySpark
spark_df = spark.createDataFrame(pandas_df)
```

---

### **Q24 (duplicate from earlier): Differences between PySpark and Pandas?**

* PySpark ‚Üí distributed, lazy evaluation, big data
* Pandas ‚Üí single machine, eager evaluation, small data

---

# üéâ **PHASE 3 ‚Äî ALL INTERVIEW ANSWERS COMPLETED**

Now practice time.

---

# üß© **PHASE 3 ‚Äî PRACTICE QUESTION ANSWERS**

Here are the answers to the practice questions you were supposed to think about.

---

### **Q1: What is a DataFrame in PySpark?**

A distributed table with rows and columns, processed by Spark across multiple machines.

---

### **Q2: What do header=True and inferSchema=True mean?**

* `header=True` ‚Üí first row is column names
* `inferSchema=True` ‚Üí Spark auto-detects data types

---

### **Q3: Write a command to select only ‚Äúname‚Äù and ‚Äúsalary‚Äù columns.**

```python
df.select("name", "salary")
```

---

### **Q4: Write a command to filter employees whose age > 40.**

```python
df.filter(df.age > 40)
```

---

### **Q5: How do you add a new column ‚Äúbonus = salary * 0.05‚Äù?**

```python
df.withColumn("bonus", df.salary * 0.05)
```

---

### **Q6: How do you drop rows with NULLs?**

```python
df.dropna()
```

---

### **Q7: Difference between union and unionByName?**

* `union()` ‚Üí matches columns by position
* `unionByName()` ‚Üí matches by column names

---

### **Q8: How do you get unique department names?**

```python
df.select("department").distinct()
```

---

# üß™ **PHASE 3 ‚Äî Coding Task Solution**

**Task:**

1. Read `employees.csv`
2. Select: name, age, department
3. Add column: tax = salary * 0.1
4. Filter: age > 25
5. Group by department and show average salary

### ‚úÖ **Solution**

```python
from pyspark.sql import SparkSession
from pyspark.sql import functions as F

# 1. Create SparkSession
spark = SparkSession.builder.appName("phase3_task").getOrCreate()

# 2. Read CSV
df = spark.read.csv("employees.csv", header=True, inferSchema=True)

# 3. Select needed columns
df_sel = df.select("name", "age", "department", "salary")

# 4. Add tax column
df_tax = df_sel.withColumn("tax", df_sel.salary * 0.1)

# 5. Filter age > 25
df_filt = df_tax.filter(df_tax.age > 25)

# 6. Group by department ‚Üí avg salary
df_group = df_filt.groupBy("department").agg(F.avg("salary").alias("avg_salary"))

df_group.show()
```

---

# üéâ **PHASE 3 is 100% complete**

Below is **PHASE 4 taught EXACTLY like Phase 1, 2, 3**:

‚úî Concept explained simply
‚úî Code examples
‚úî Interview answers for all questions
‚úî Practice questions
‚úî Mini coding test

Absolutely beginner-friendly, zero advanced content.

---

# üîµ **PHASE 4 ‚Äî Transformations vs Actions**

## üéØ **Goal:**

Understand **how Spark thinks**, **how jobs run**, and **why lazy evaluation makes Spark fast**.

This phase is *small but extremely important* ‚Äî interviewers always ask about this.

---

# ‚úÖ **4.1 What are Transformations? (Very Important)**

### üß† Concept

Transformations:

‚úî Do **NOT** execute immediately
‚úî Only **prepare** the steps
‚úî Creates a **logical plan** in Spark
‚úî Execution happens later when an action is called

Think of transformations like:

> ‚ÄúWriting cooking steps but not actually cooking yet.‚Äù

---

### üß™ Example

```python
df2 = df.select("name", "age")
df3 = df2.filter(df2.age > 25)
df4 = df3.withColumn("double_age", df3.age * 2)
```

None of the above **runs immediately**.

Spark is just building a plan.

---

# ‚ú® **4.2 What are Actions?**

### üß† Concept

Actions:

‚úî **Trigger the real execution**
‚úî Force Spark to compute results
‚úî Return output to driver or print results

Think:

> ‚ÄúNow start cooking ‚Äî execute all steps.‚Äù

---

### üß™ Example

```python
df4.show()        # executes full pipeline
df4.count()       # counts rows
rows = df4.collect()  # brings all rows to driver
```

---

# üî• **4.3 Lazy Evaluation (Most Important Concept)**

### üß† Concept

Lazy evaluation means:

‚úî Transformations do NOT execute immediately
‚úî Spark waits until an **action** is called
‚úî Then Spark optimizes everything using **Catalyst Optimizer**
‚úî This makes Spark **much faster**

---

### üß† Beginner Explanation

Spark does this for speed:

> ‚ÄúDon‚Äôt run step-by-step.
> Wait until the last moment, optimize everything, then run once.‚Äù

---

# üîó **INTERVIEW QUESTIONS & PERFECT ANSWERS**

Below are the answers for all questions in this phase.

---

### **Q31. What is a transformation in PySpark?**

A transformation is an operation that creates a new DataFrame without executing immediately.

Examples:
`select()`, `filter()`, `withColumn()`, `groupBy()`

---

### **Q32. What is an action in PySpark?**

An action is an operation that triggers execution of the entire plan.

Examples:
`show()`, `count()`, `collect()`

---

### **Q33. Give examples of transformations.**

* `select()`
* `filter()`
* `groupBy()`
* `withColumn()`
* `orderBy()`
* `dropna()`

---

### **Q34. Give examples of actions.**

* `show()`
* `collect()`
* `count()`
* `take()`
* `head()`

---

### **Q35. What is lazy evaluation in PySpark?**

Transformations don‚Äôt run immediately.
Spark waits until an action is called to execute the full plan.

---

### **Q36. Why does Spark use lazy evaluation?**

Because it:

‚úî Optimizes the job
‚úî Reduces unnecessary computation
‚úî Makes big data processing faster

---

# üß© **PHASE 4 ‚Äî PRACTICE QUESTIONS**

Try answering these before checking the solutions.

---

### **Q1. Is select() a transformation or action? Why?**

---

### **Q2. When does a filter() actually run?**

---

### **Q3. What happens if you write transformations but never call show()?**

---

### **Q4. Write one transformation and one action.**

---

### **Q5. Explain lazy evaluation in 1 line.**

---

### **Q6. Which line triggers execution?**

```python
df2 = df.filter(df.age > 20)
df3 = df2.withColumn("tax", df.salary * 0.1)
df4 = df3.groupBy("department")
df4.show()
```

---

# üß™ **PHASE 4 ‚Äî CODE PRACTICE**

### **Task: Identify transformations vs actions**

```python
df2 = df.select("name", "age")
df3 = df2.filter(df2.age > 30)
df4 = df3.withColumn("bonus", df3.salary * 0.2)
total = df4.count()
df4.show()
```

Questions:

1. Which lines are transformations?
2. Which lines are actions?
3. Which line triggers actual execution?

---

# üéâ **PHASE 4 Completed**

Once you answer the practice questions, say:
Here are the **perfect beginner-friendly answers** for all **Phase 4 practice questions + code identification questions.**
These are the exact answers interviewers expect.

---

# ‚úÖ **PHASE 4 ‚Äî PRACTICE QUESTION ANSWERS**

---

### **Q1. Is select() a transformation or action? Why?**

**Answer:**
`select()` is a **transformation** because it creates a new DataFrame but does NOT trigger execution.

---

### **Q2. When does a filter() actually run?**

**Answer:**
`filter()` runs only when an **action** (like `show()` or `count()`) is called.

---

### **Q3. What happens if you write transformations but never call show()?**

**Answer:**
Nothing gets executed.
Spark only builds a plan; it doesn‚Äôt run any computation until an action is triggered.

---

### **Q4. Write one transformation and one action.**

**Transformation:**

```python
df2 = df.filter(df.age > 30)
```

**Action:**

```python
df2.count()
```

---

### **Q5. Explain lazy evaluation in 1 line.**

**Answer:**
Lazy evaluation means Spark waits for an **action** to run all transformations together efficiently.

---

### **Q6. Which line triggers execution?**

```python
df2 = df.filter(df.age > 20)
df3 = df2.withColumn("tax", df.salary * 0.1)
df4 = df3.groupBy("department")
df4.show()
```

**Answer:**
`df4.show()` is the **action** ‚Üí it triggers execution of *all previous transformations*.

---

# üß™ **PHASE 4 ‚Äî CODE PRACTICE ANSWERS**

Given code:

```python
df2 = df.select("name", "age")
df3 = df2.filter(df2.age > 30)
df4 = df3.withColumn("bonus", df3.salary * 0.2)
total = df4.count()
df4.show()
```

---

## **1. Which lines are transformations?**

‚úî `df2 = df.select("name", "age")`
‚úî `df3 = df2.filter(df2.age > 30)`
‚úî `df4 = df3.withColumn("bonus", df3.salary * 0.2)`

---

## **2. Which lines are actions?**

‚úî `total = df4.count()`
‚úî `df4.show()`

---

## **3. Which line triggers actual execution?**

**Answer:**
Both:

* `df4.count()`
* `df4.show()`

trigger execution (each action separately runs the entire plan).

---

# üéâ **PHASE 4 completed 100%**

Great ‚Äî here is **PHASE 5 explained exactly like previous phases**, including:

‚úî Simple explanation
‚úî Code
‚úî Interview answers
‚úî Practice questions
‚úî Small coding task

No advanced content ‚Äî fully beginner/interview-level only.

---

# üîµ **PHASE 5 ‚Äî Basic Operations & Aggregations**

## üéØ **Goal:**

Learn the most common real-world DataFrame operations you‚Äôll definitely face in interviews & assignments.

These operations are ALWAYS asked in practical rounds.

---

# ‚úÖ **5.1 Adding New Column (withColumn)**

### üß† Concept

`withColumn()` is used to:

‚úî Add a new column
‚úî Modify an existing column

### üß™ Example: Add bonus = 10% of salary

```python
df = df.withColumn("bonus", df.salary * 0.1)
```

This creates a new column **bonus** for every row.

---

# üîó **Interview Question & Answer**

### **Q37. How do you add a new column using withColumn?**

Using `withColumn("new_column", expression)`.

Example:

```python
df.withColumn("bonus", df.salary * 0.1)
```

---

### **Q38. How do you calculate a bonus or percentage column?**

**Answer:**

```python
df.withColumn("bonus", df.salary * 0.1)
```

OR for percentage:

```python
df.withColumn("tax", df.salary * 0.05)
```

---

# ‚úÖ **5.2 Grouping & Aggregations (Most Asked)**

### üß† Concept

You can group rows by a column and apply aggregations like:

* avg
* sum
* max
* min
* count

### üß™ Example:

```python
from pyspark.sql import functions as F

df.groupBy("department").agg(
    F.avg("salary").alias("avg_salary"),
    F.max("salary").alias("max_salary"),
    F.sum("salary").alias("total_salary")
).show()
```

---

# üîó **Interview Questions & Answers**

### **Q39. How do you group data in PySpark?**

Using `groupBy()` and `agg()`:

```python
df.groupBy("department").agg(F.avg("salary"))
```

---

### **Q40. How do you calculate average, sum, max in a group?**

```python
df.groupBy("department").agg(
    F.avg("salary"),
    F.sum("salary"),
    F.max("salary")
)
```

---

# üéâ **All 4 Phase 5 Interview Questions Completed**

Now test your understanding.

---

# üß© **PHASE 5 ‚Äî Practice Questions (Answer These)**

### **Q1:** Write code to add a column: `tax = salary * 0.15`

---

### **Q2:** Write code to group by ‚Äúcity‚Äù and calculate avg(age).

---

### **Q3:** Write code to find total salary for each department.

---

### **Q4:** What does withColumn() do?

(One line explanation)

---

### **Q5:** What is the output of this?

```python
df.withColumn("double_salary", df.salary * 2)
```

---

# üß™ **Mini Coding Task**

Dataset columns:

`id, name, age, salary, department`

### **Task:**

1. Add column: `hra = salary * 0.2`
2. Filter rows: age > 30
3. Group by department and calculate:

   * avg salary
   * max salary
   * min salary
4. Show the result
Here are the **perfect beginner-friendly answers** for all **Phase 5 practice questions + coding task solution** ‚Äî exactly how you should answer in an interview.

---

# ‚úÖ **PHASE 5 ‚Äî Practice Question Answers**

---

### **Q1: Write code to add a column: `tax = salary * 0.15`**

```python
df = df.withColumn("tax", df.salary * 0.15)
```

---

### **Q2: Write code to group by ‚Äúcity‚Äù and calculate avg(age).**

```python
from pyspark.sql import functions as F

df.groupBy("city").agg(F.avg("age").alias("avg_age")).show()
```

---

### **Q3: Write code to find total salary for each department.**

```python
from pyspark.sql import functions as F

df.groupBy("department").agg(F.sum("salary").alias("total_salary")).show()
```

---

### **Q4: What does withColumn() do? (One line)**

**Answer:**
`withColumn()` is used to create a new column or update an existing column in a DataFrame.

---

### **Q5: What is the output of this?**

```python
df.withColumn("double_salary", df.salary * 2)
```

**Answer:**
It creates a new column **double_salary** where each row contains salary √ó 2.

---

# üß™ **PHASE 5 ‚Äî Mini Coding Task Solution**

Dataset columns:
`id, name, age, salary, department`

### **Task Requirements + Final Answer**

---

### **1. Add column: `hra = salary * 0.2`**

```python
df1 = df.withColumn("hra", df.salary * 0.2)
```

---

### **2. Filter rows: age > 30**

```python
df2 = df1.filter(df1.age > 30)
```

---

### **3. Group by department ‚Üí avg, max, min salary**

```python
from pyspark.sql import functions as F

df3 = df2.groupBy("department").agg(
    F.avg("salary").alias("avg_salary"),
    F.max("salary").alias("max_salary"),
    F.min("salary").alias("min_salary")
)
```

---

### **4. Show result**

```python
df3.show()
```

---

# üéâ **FINAL FULL SOLUTION (Combined Clean Code)**

```python
from pyspark.sql import SparkSession
from pyspark.sql import functions as F

spark = SparkSession.builder.appName("phase5_task").getOrCreate()

# Assume df is already loaded
# df = spark.read.csv("employees.csv", header=True, inferSchema=True)

# 1. Add HRA column
df1 = df.withColumn("hra", df.salary * 0.2)

# 2. Filter age > 30
df2 = df1.filter(df1.age > 30)

# 3. Group and aggregate
df3 = df2.groupBy("department").agg(
    F.avg("salary").alias("avg_salary"),
    F.max("salary").alias("max_salary"),
    F.min("salary").alias("min_salary")
)

# 4. Show results
df3.show()
```

---
Here is **PHASE 6 explained exactly like earlier phases**, including:

‚úî Concept
‚úî Code
‚úî Interview answers
‚úî Practice questions
‚úî Coding test

No advanced topics ‚Äî only beginner-friendly and interview-ready content.

---

# üîµ **PHASE 6 ‚Äî Joins (Very Important)**

Joins are **the #1 topic asked in PySpark interviews**
‚Üí If you learn this phase properly, you can clear most basic PySpark rounds.

---

# ‚úÖ **6.0 Understanding Joins**

You will usually have two DataFrames:

### **Employees Table**

```
emp(id, name, dept_id, salary)
```

### **Departments Table**

```
dept(id, dept_name)
```

We join them on:

```
emp.dept_id == dept.id
```

---

# ‚úÖ **6.1 Inner Join**

### üß† Concept

Returns **only matching rows** from both DataFrames.

### üß™ Code

```python
emp.join(dept, emp.dept_id == dept.id, "inner")
```

If dept_id doesn‚Äôt match ‚Üí row is NOT included.

---

# ‚úÖ **6.2 Left Join**

### üß† Concept

Return:

‚úî All rows from LEFT (emp)
‚úî Matching rows from right (dept)
‚úî If no match ‚Üí dept columns = NULL

### üß™ Code

```python
emp.join(dept, emp.dept_id == dept.id, "left")
```

---

# ‚úÖ **6.3 Right Join**

### üß† Concept

Opposite of left join:

‚úî All rows from RIGHT (dept)
‚úî Matching rows from emp
‚úî If no match ‚Üí emp columns = NULL

```python
emp.join(dept, emp.dept_id == dept.id, "right")
```

---

# ‚úÖ **6.4 Full Join**

### üß† Concept

Return:

‚úî All rows from both sides
‚úî Match when possible
‚úî No match ‚Üí NULLs on missing side

### üß™ Code

```python
emp.join(dept, emp.dept_id == dept.id, "full")
```

---

# üîó **INTERVIEW QUESTIONS & PERFECT ANSWERS**

---

### **Q41. What is a join in PySpark?**

A join combines two DataFrames based on a condition, usually matching keys.

---

### **Q42. What are the types of joins in PySpark?**

‚úî inner
‚úî left
‚úî right
‚úî full
‚úî (Others exist, but these 4 are enough for beginner interviews)

---

### **Q43. How do you perform a join in PySpark?**

```python
emp.join(dept, emp.dept_id == dept.id, "inner")
```

---

### **Q44. What is a left join? Explain with example.**

A **left join** returns all rows from the left DataFrame and matching rows from the right.
If no match exists, right-side columns become NULL.

**Example:**

```python
emp.join(dept, emp.dept_id == dept.id, "left")
```

---

# üéâ **All Phase 6 Interview Answers DONE**

Now practice for exam + interview.

---

# üß© **PHASE 6 ‚Äî PRACTICE QUESTIONS**

### **Q1:** Write code to inner join `emp` and `dept`.

---

### **Q2:** What does a left join return?

---

### **Q3:** Which join will return NULLs on BOTH sides?

---

### **Q4:** What is the difference between inner and full join?

---

### **Q5:** True or False?

Right join returns all rows from the left DataFrame.

---

### **Q6:** Write a full join between these:

```
orders(order_id, user_id)
users(id, name)
```

---

# üß™ **PHASE 6 ‚Äî Coding Task**

### **Task:**

Given two DataFrames:

```
orders(order_id, user_id, amount)
users(id, name)
```

Do the following:

1. Perform a **left join** between orders and users
2. Show `order_id, name, amount`
3. Display NULL where name is missing

---
Here are the **complete, beginner-friendly answers** for all **Phase 6 practice questions + coding task**, exactly in the style interviewers expect.

---

# ‚úÖ **PHASE 6 ‚Äî PRACTICE QUESTION ANSWERS**

---

### **Q1: Write code to inner join `emp` and `dept`.**

```python
emp.join(dept, emp.dept_id == dept.id, "inner")
```

---

### **Q2: What does a left join return?**

**Answer:**
A left join returns:

‚úî All rows from the left DataFrame
‚úî Matching rows from the right
‚úî If no match ‚Üí right columns become NULL

---

### **Q3: Which join will return NULLs on BOTH sides?**

**Answer:**
**Full join**

---

### **Q4: What is the difference between inner and full join?**

* **Inner Join:** Returns only matching rows from both sides.
* **Full Join:** Returns all rows from both sides; unmatched rows contain NULLs.

---

### **Q5: True or False?

Right join returns all rows from the left DataFrame.**

**Answer:**
‚ùå **False**
Right join returns all rows from the **right** DataFrame.

---

### **Q6: Write a full join between orders and users.**

```python
orders.join(users, orders.user_id == users.id, "full")
```

---

# üß™ **PHASE 6 ‚Äî CODING TASK SOLUTION**

### **Task:**

DataFrames:

```
orders(order_id, user_id, amount)
users(id, name)
```

Goal:

1. Left join
2. Select: order_id, name, amount
3. Show NULL when no matching user

---

# ‚úÖ **Final Code Solution**

```python
# 1. LEFT JOIN orders with users
result = orders.join(
    users,
    orders.user_id == users.id,
    "left"
)

# 2. Select necessary columns
final_df = result.select(
    "order_id",
    "name",
    "amount"
)

# 3. Show result
final_df.show()
```

---

# üéâ **PHASE 6 Completed 100%**

Here is **PHASE 7 explained exactly like previous phases**, including:

‚úî Clear concept
‚úî Code
‚úî Interview answers
‚úî Practice questions
‚úî Coding task

Beginner-friendly, no advanced topics.

---

# üîµ **PHASE 7 ‚Äî Handling NULL Values**

Handling NULLs is extremely important ‚Äî in real data almost every dataset has missing values.

---

# ‚úÖ **7.1 Dropping NULL Values**

### üß† Concept

You drop rows that contain **any NULL** or NULL in **specific columns**.

### üß™ Example 1 ‚Äî Drop rows with ANY NULL:

```python
df2 = df.dropna()
```

---

### üß™ Example 2 ‚Äî Drop rows with NULL in specific columns:

```python
df2 = df.dropna(subset=["age", "salary"])
```

---

# ‚úÖ **7.2 Filling NULL Values**

### üß† Concept

Use `fillna()` to replace NULLs with default values.

### üß™ Example:

```python
df2 = df.fillna({"age": 0, "salary": 0})
```

You can fill:

* Age ‚Üí 0
* Salary ‚Üí 0
* Department ‚Üí "Unknown"

---

# üß† **7.3 Why is NULL Handling Important?**

NULLs cause:

‚ùå Wrong aggregations (avg, sum, max)
‚ùå Errors in joins
‚ùå Wrong calculations (salary * 0.1)
‚ùå Incorrect ML results

So cleaning NULLs is ALWAYS done in ETL.

Example:

If salary = NULL ‚Üí bonus calculation ‚Üí becomes NULL ‚Üí wrong output.

---

# üîó **INTERVIEW QUESTIONS & PERFECT ANSWERS**

---

### **Q45. How do you drop rows with NULL values?**

```python
df.dropna()
```

Or specific columns:

```python
df.dropna(subset=["age"])
```

---

### **Q46. How do you fill NULL values?**

```python
df.fillna({"age": 0, "salary": 0})
```

---

### **Q47. Why is NULL handling important?**

Because NULL values can cause incorrect results in aggregations, joins, and calculations. Cleaning NULLs ensures accurate and reliable data processing.

---

# üéâ **All PHASE 7 interview answers DONE**

Now practice time.

---

# üß© **PHASE 7 ‚Äî PRACTICE QUESTIONS**

Try answering these:

### **Q1:** Remove rows where *any* value is NULL.

---

### **Q2:** Remove rows only when ‚Äúage‚Äù is NULL.

---

### **Q3:** Fill NULL salary values with 50000.

---

### **Q4:** Fill NULL department values with ‚ÄúUnknown‚Äù.

---

### **Q5:** Why is handling NULL important during joins?

---

### **Q6:** If salary = NULL, what will happen when you compute `salary * 0.1`?

---

---

# üß™ **PHASE 7 ‚Äî Coding Task**

Dataset columns:

`id, name, age, salary, department`

### **Task:**

1. Remove rows where salary is NULL
2. Replace missing age with 0
3. Replace missing department with "Not Available"
4. Show result

Here are the **perfect answers** for all **Phase 7 practice questions + coding task**, exactly in interview style.

---

# ‚úÖ **PHASE 7 ‚Äî Practice Question Answers**

---

### **Q1: Remove rows where *any* value is NULL.**

```python
df2 = df.dropna()
```

---

### **Q2: Remove rows only when ‚Äúage‚Äù is NULL.**

```python
df2 = df.dropna(subset=["age"])
```

---

### **Q3: Fill NULL salary values with 50000.**

```python
df2 = df.fillna({"salary": 50000})
```

---

### **Q4: Fill NULL department values with ‚ÄúUnknown‚Äù.**

```python
df2 = df.fillna({"department": "Unknown"})
```

---

### **Q5: Why is handling NULL important during joins?**

**Answer:**
Because NULLs can cause missing matches, produce incorrect join results, or generate NULL outputs in the final merged dataset.

---

### **Q6: If salary = NULL, what happens when you compute `salary * 0.1`?**

**Answer:**
The result becomes **NULL**, because any calculation with NULL results in NULL.

---

# üéâ Perfect ‚Äî practice questions done!

---

# üß™ **PHASE 7 ‚Äî Coding Task Solution**

Dataset:

`id, name, age, salary, department`

### **Task Requirements + Solution**

---

### **1. Remove rows where salary is NULL**

```python
df1 = df.dropna(subset=["salary"])
```

---

### **2. Replace missing age with 0**

```python
df2 = df1.fillna({"age": 0})
```

---

### **3. Replace missing department with "Not Available"**

```python
df3 = df2.fillna({"department": "Not Available"})
```

---

### **4. Show the final result**

```python
df3.show()
```

---

# üåü **Final Full Solution (Clean Code)**

```python
# 1. Drop rows where salary is NULL
df1 = df.dropna(subset=["salary"])

# 2. Fill missing age
df2 = df1.fillna({"age": 0})

# 3. Fill missing department
df3 = df2.fillna({"department": "Not Available"})

# 4. Show
df3.show()
```

---

Here is **PHASE 8 explained exactly like earlier phases**, with:

‚úî Concept
‚úî Simple examples
‚úî Interview answers
‚úî Practice questions
‚úî Coding task

Everything is beginner-friendly and 100% interview-oriented.

---

# üîµ **PHASE 8 ‚Äî File Formats & Schema**

File formats are an **easy but high-frequency interview topic**, especially for apprenticeship & fresher rounds.

---

# ‚úÖ **8.1 Supported File Formats in PySpark**

PySpark commonly works with:

### ‚úî CSV

Text-based, row-oriented, slow for big data.

### ‚úî JSON

Hierarchical / nested data.

### ‚úî Parquet

Columnar, compressed, best for big data processing.

### ‚úî ORC

Also columnar, optimized for Hadoop.

### ‚úî Avro

Used in streaming / messaging systems.

---

# üß™ **How to read them (concept-level)**

### **CSV**

```python
df = spark.read.csv("data.csv", header=True, inferSchema=True)
```

---

### **JSON**

```python
df = spark.read.json("data.json")
```

---

### **Parquet**

```python
df = spark.read.parquet("data.parquet")
```

---

### **Avro** (if supported)

```python
df = spark.read.format("avro").load("data.avro")
```

---

# üî• **8.2 Why Parquet Is the Best Format**

### ‚úî Columnar

Data stored column-by-column, not row-by-row.

### ‚úî Highly compressed

Takes far less space on disk.

### ‚úî Faster queries

Spark reads only required columns rather than entire rows.

### ‚úî Preferred for big data & analytics

Used in almost all production pipelines.

---

# üü¶ **8.3 JSON Format**

JSON is used for:

‚úî Nested data
‚úî API data
‚úî Logs
‚úî Event data

Spark automatically handles nested structures (concept-level only for now).

---

# üü© **8.4 Schema in PySpark**

### üß† What is schema?

Schema = structure of your DataFrame
(columns + datatypes)

Example schema:

```
root
 |-- name: string
 |-- age: integer
 |-- salary: double
```

### üß™ How to see schema?

```python
df.printSchema()
```

### üí° Why schema matters?

‚úî Ensures type correctness
‚úî Improves performance
‚úî Prevents errors in joins
‚úî Helps Spark optimize queries

---

# üîó **INTERVIEW QUESTIONS & PERFECT ANSWERS**

---

### **Q48. What file formats does PySpark support?**

CSV, JSON, Parquet, ORC, Avro.

---

### **Q49. Why is Parquet faster than CSV?**

Parquet is columnar, compressed, and Spark reads only required columns, making it faster and more efficient.

---

### **Q50. What is JSON used for?**

For nested or hierarchical data like API responses, logs, and metadata.

---

### **Q51. Which format is best for big data use cases?**

**Parquet** ‚Äî because it is columnar, compressed, and optimized for analytics.

---

### **Q52. How do you work with JSON, Parquet, Avro in PySpark?**

Use:

```python
spark.read.json()
spark.read.parquet()
spark.read.format("avro").load()
```

---

### **Q53. What is a DataFrame schema and why is it important?**

Schema defines the structure and data types of a DataFrame.
It is important for performance, correctness, and preventing type errors during transformations and joins.

---

# üéâ **Phase 8 Interview Answers Completed**

Now test your understanding.

---

# üß© **PHASE 8 ‚Äî Practice Questions**

### **Q1:** Why is Parquet preferred over CSV?

---

### **Q2:** Write code to read "products.json" in PySpark.

---

### **Q3:** What kind of data does JSON handle well?

---

### **Q4:** Write code to read a Parquet file.

---

### **Q5:** What does a schema contain?

---

### **Q6:** Why is schema important in PySpark?

---

---

# üß™ **PHASE 8 ‚Äî Coding Task**

Dataset files:

```
employees.csv
employees.parquet
employees.json
```

### **Task:**

1. Read all three files into DataFrames
2. Print schema of each
3. Identify which format loads fastest (conceptually)
4. Explain why

---
Here are the **complete Phase 8 answers**, including **practice questions + coding task solution**, written exactly in beginner-friendly, interview-ready style.

---

# ‚úÖ **PHASE 8 ‚Äî Practice Question Answers**

---

### **Q1: Why is Parquet preferred over CSV?**

**Answer:**
Because Parquet is **columnar, compressed, faster, and more efficient** than CSV.
Spark reads only required columns instead of whole rows, which reduces scanning time.

---

### **Q2: Write code to read `"products.json"` in PySpark.**

```python
df = spark.read.json("products.json")
```

---

### **Q3: What kind of data does JSON handle well?**

**Answer:**
JSON works well with **nested** or **hierarchical** data such as:

* API responses
* Logs
* Event data
* Metadata

---

### **Q4: Write code to read a Parquet file.**

```python
df = spark.read.parquet("data.parquet")
```

---

### **Q5: What does a schema contain?**

**Answer:**
Schema contains:

* Column names
* Data types
* Structure of the DataFrame

Example:

```
name: string  
age: integer  
salary: double
```

---

### **Q6: Why is schema important in PySpark?**

**Answer:**
Schema is important because it ensures:

‚úî Correct data types
‚úî Faster execution
‚úî Fewer errors in joins
‚úî Better optimization by Spark

---

# üéâ Practice Questions DONE PERFECTLY

Now the coding task.

---

# üß™ **PHASE 8 ‚Äî Coding Task Solution**

### **Task:**

Read:

* `employees.csv`
* `employees.parquet`
* `employees.json`

Then print schema of each and compare which format loads fastest.

---

# ‚úÖ **Final Full Code Solution**

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("phase8_formats").getOrCreate()

# 1. Read CSV
df_csv = spark.read.csv("employees.csv", header=True, inferSchema=True)

# 2. Read Parquet
df_parquet = spark.read.parquet("employees.parquet")

# 3. Read JSON
df_json = spark.read.json("employees.json")

# Print schemas
print("CSV Schema:")
df_csv.printSchema()

print("Parquet Schema:")
df_parquet.printSchema()

print("JSON Schema:")
df_json.printSchema()
```

---

# üß† **3. Identify which is fastest (conceptual answer)**

### ‚úî **Parquet loads fastest**

Because:

* It‚Äôs columnar
* Compressed
* Optimized for Spark
* Stores data + schema together

CSV and JSON are slower because they:

* Are row-based
* Not compressed
* Require parsing
* Need type inference

---

# üéâ **PHASE 8 is fully completed**

Here is **PHASE 9 explained exactly like previous phases**, including:

‚úî Simple concepts
‚úî Code
‚úî Interview answers
‚úî Practice questions
‚úî Coding task

Beginner-friendly & perfect for apprenticeship interviews.

---

# üîµ **PHASE 9 ‚Äî SQL in PySpark**

SQL with PySpark is very powerful because you can use normal SQL syntax directly on DataFrames.

---

# ‚úÖ **9.1 Creating a Temporary SQL View**

### üß† Concept

A temp view allows you to run SQL queries **as if the DataFrame were a SQL table**.

### üß™ Code

```python
df.createOrReplaceTempView("emp")
```

This creates a table named `"emp"` that you can query using SQL.

---

# ‚úÖ **9.2 Running SQL Queries**

### üß† Concept

Use `spark.sql()` to run SQL statements.

### üß™ Example

```python
result = spark.sql("SELECT name, age FROM emp WHERE age > 30")
result.show()
```

---

# üî• What SQL can do in PySpark?

‚úî SELECT columns
‚úî WHERE filtering
‚úî GROUP BY
‚úî ORDER BY
‚úî Aggregations (SUM, AVG, MAX, MIN)

Everything you know from SQL works here.

---

# üîó **INTERVIEW QUESTIONS & PERFECT ANSWERS**

---

### **Q54. How do you create a temporary SQL table in PySpark?**

```python
df.createOrReplaceTempView("emp")
```

---

### **Q55. How do you run SQL queries using spark.sql?**

```python
spark.sql("SELECT * FROM emp")
```

---

### **Q56. How do you filter data using SQL queries in PySpark?**

```python
spark.sql("SELECT * FROM emp WHERE age > 30")
```

---

### **Q57. How do you select columns using SQL in PySpark?**

```python
spark.sql("SELECT name, age FROM emp")
```

---

# üéâ PHASE 9 ‚Äî Interview Answers Completed

Now practice to strengthen your skills.

---

# üß© **PHASE 9 ‚Äî Practice Questions**

Try answering these yourself:

---

### **Q1:** Create a temp view called `"students"` from DataFrame `df_students`.

---

### **Q2:** Write SQL to select `name` and `city` from `"students"`.

---

### **Q3:** Write SQL to get employees with salary > 50000.

---

### **Q4:** Write SQL to get average salary per department.

---

### **Q5:** Write SQL to get only distinct department names.

---

---

# üß™ **PHASE 9 ‚Äî Coding Task**

You have a DataFrame:

```
emp(id, name, age, salary, department)
```

### **Task:**

1. Create temp view `"emp"`
2. Run SQL to:

   * Select name, salary
   * Filter age > 25
   * Group by department
   * Show average salary

---

Below are the **perfect beginner-friendly answers** for all **Phase 9 practice questions + coding task** ‚Äî exactly as expected in interviews.

---

# ‚úÖ **PHASE 9 ‚Äî Practice Question Answers**

---

### **Q1: Create a temp view called `"students"` from DataFrame `df_students`.**

```python
df_students.createOrReplaceTempView("students")
```

---

### **Q2: Write SQL to select `name` and `city` from `"students"`.**

```python
spark.sql("SELECT name, city FROM students")
```

---

### **Q3: Write SQL to get employees with salary > 50000.**

```python
spark.sql("SELECT * FROM emp WHERE salary > 50000")
```

---

### **Q4: Write SQL to get average salary per department.**

```python
spark.sql("SELECT department, AVG(salary) AS avg_salary FROM emp GROUP BY department")
```

---

### **Q5: Write SQL to get only distinct department names.**

```python
spark.sql("SELECT DISTINCT department FROM emp")
```

---

# üéâ Practice Questions DONE!

Now the coding task solution.

---

# üß™ **PHASE 9 ‚Äî Coding Task Solution**

DataFrame:

```
emp(id, name, age, salary, department)
```

### **Task Requirements + Final Solution**

---

### **1. Create temp view "emp"**

```python
df.createOrReplaceTempView("emp")
```

---

### **2. Run SQL to:**

* Select **name**, **salary**
* Filter **age > 25**
* Group by **department**
* Show **average salary**

### ‚úÖ **Final SQL Query**

```python
result = spark.sql("""
    SELECT 
        department,
        AVG(salary) AS avg_salary
    FROM emp
    WHERE age > 25
    GROUP BY department
""")

result.show()
```

---

# üéâ PHASE 9 COMPLETED 100%
Here is **PHASE 10 explained exactly like previous phases**, with:

‚úî Concept
‚úî Clean code
‚úî Interview answers
‚úî Practice questions
‚úî Mini coding task (your last test before finishing PySpark basics)

Beginner-friendly. No advanced content.

---

# üîµ **PHASE 10 ‚Äî Mini ETL Task (Must Be Confident)**

This is the **same coding task** almost every company gives for apprenticeships & fresher PySpark interviews.

You must be able to:

‚úî Write the ETL steps
‚úî Explain them verbally
‚úî Show code
‚úî Answer related questions

Let‚Äôs go step by step.

---

# ‚úÖ **10.1 FILTER ‚Üí Employees age > 30**

### üß† Concept

Keep only rows where age is greater than 30.

### üß™ Code

```python
df2 = df.filter(df.age > 30)
```

Equivalent using `where()`:

```python
df2 = df.where(df.age > 30)
```

---

# ‚úÖ **10.2 TRANSFORM ‚Üí Add Bonus Column**

### üß† Concept

Add a new column:
**bonus = salary * 0.1**

### üß™ Code

```python
df3 = df2.withColumn("bonus", df2.salary * 0.1)
```

---

# ‚úÖ **10.3 GROUP ‚Üí Average Salary by Age**

### üß† Concept

Group rows by age and compute average salary.

### üß™ Code

```python
from pyspark.sql import functions as F

df4 = df3.groupBy("age").agg(
    F.avg("salary").alias("avg_salary")
)

df4.show()
```

---

# ‚úÖ **10.4 Explain ETL Steps Clearly (Interview-Perfect Answer)**

When interviewer asks:

**‚ÄúExplain what ETL you performed.‚Äù**

You say:

**E (Extract)**

* Data loaded from CSV/Parquet into a DataFrame using `spark.read`.

**T (Transform)**

* Filtered employees with age > 30
* Added a new calculated column: bonus = salary * 0.1
* Grouped by age and calculated average salary

**L (Load)**

* Final DataFrame can be written back to CSV/Parquet or passed to reporting.

This is **exactly** how you explain it in a face-to-face round.

---

# üîó **PHASE 10 ‚Äî INTERVIEW QUESTIONS & PERFECT ANSWERS**

---

### **Q58. How do you filter employees whose age > 30?**

```python
df.filter(df.age > 30)
```

---

### **Q59. How do you add a bonus column = 10% salary?**

```python
df.withColumn("bonus", df.salary * 0.1)
```

---

### **Q60. How do you group by age and find average salary?**

```python
df.groupBy("age").agg(F.avg("salary"))
```

---

### **Q61. Explain the ETL steps you performed.**

**Answer:**
I extracted the data from CSV into a DataFrame.
Then I transformed it by filtering employees older than 30, adding a bonus column, and grouping by age to compute average salary.
Finally, the result can be loaded into a new file or used in reports.

---

# üéâ **All PHASE 10 interview answers DONE**

Now let‚Äôs practice.

---

# üß© **PHASE 10 ‚Äî Practice Questions**

Try answering these yourself:

---

### **Q1:** Write code to filter rows where salary > 60000.

---

### **Q2:** Add a new column `tax = salary * 0.05`.

---

### **Q3:** Group by department and find total salary.

---

### **Q4:** Explain ETL in 3 lines.

---

### **Q5:** What happens if salary contains NULL when computing bonus?

---

---

# üß™ **PHASE 10 ‚Äî Coding Task (Final ETL Test)**

Dataset:

`id, name, age, salary, department`

### **Task:**

1. Filter employees age >= 28
2. Add HRA = salary * 0.2
3. Add PF = salary * 0.12
4. Group by department ‚Üí find avg, max, min salary
5. Show final output

---

Here are the **answers for all Phase 10 practice questions + final ETL coding task**, written exactly in beginner-friendly + interview-ready format.

---

# ‚úÖ **PHASE 10 ‚Äî Practice Question Answers**

---

# üß© **Q1. Write code to filter rows where salary > 60000.**

```python
df_high_salary = df.filter(df.salary > 60000)
df_high_salary.show()
```

---

# üß© **Q2. Add a new column `tax = salary * 0.05`.**

```python
df_with_tax = df.withColumn("tax", df.salary * 0.05)
df_with_tax.show()
```

---

# üß© **Q3. Group by department and find total salary.**

```python
from pyspark.sql import functions as F

df_dept_total = df.groupBy("department").agg(
    F.sum("salary").alias("total_salary")
)
df_dept_total.show()
```

---

# üß© **Q4. Explain ETL in 3 lines. (Interview Answer)**

**E (Extract):** Load data from CSV/JSON/Parquet into a PySpark DataFrame.
**T (Transform):** Apply operations like filtering, adding columns, joining, grouping.
**L (Load):** Save the final output to CSV/Parquet or pass to another system.

---

# üß© **Q5. What happens if salary contains NULL when computing bonus?**

**Answer:**
If salary is NULL, then:

```python
bonus = salary * 0.1
```

will also become **NULL**, because any calculation with NULL results in NULL.

To avoid this, we can fill NULL salaries before computing bonus:

```python
df = df.fillna({"salary": 0})
```

---

# üéØ **PHASE 10 ‚Äî Final ETL Task (FULL ANSWER)**

Dataset columns:

`id, name, age, salary, department`

---

# üîµ **Step 1 ‚Äî Filter employees age >= 28**

```python
df1 = df.filter(df.age >= 28)
```

---

# üîµ **Step 2 ‚Äî Add HRA = salary * 0.2**

```python
df2 = df1.withColumn("HRA", df1.salary * 0.2)
```

---

# üîµ **Step 3 ‚Äî Add PF = salary * 0.12**

```python
df3 = df2.withColumn("PF", df2.salary * 0.12)
```

---

# üîµ **Step 4 ‚Äî Group by department ‚Üí avg, max, min salary**

```python
from pyspark.sql import functions as F

df4 = df3.groupBy("department").agg(
    F.avg("salary").alias("avg_salary"),
    F.max("salary").alias("max_salary"),
    F.min("salary").alias("min_salary")
)
```

---

# üîµ **Step 5 ‚Äî Show final output**

```python
df4.show()
```

---

# üß† **Interview Explanation (Use This)**

‚ÄúIn my ETL pipeline, I first **extracted** the data into a DataFrame using Spark.
Then I **transformed** it by filtering employees aged 28+, adding calculated columns like HRA and PF, and grouping by department to get summary metrics.
Finally, the results can be **loaded** into Parquet or CSV for reporting.‚Äù

---














  


