Good morning/afternoon. My name is Md Tabish, and I am from Kolkata, West Bengal. I completed my B.Tech in Computer Science and Information Technology from the University of Engineering and Management.

Over the past year, I have focused on building a strong foundation in Python development and backend engineering. My technical experience includes developing backend services using Flask, building REST APIs, and working with Oracle SQL for table design, writing efficient queries, and integrating database operations with backend APIs.

As part of my learning journey, I built two major projects. The first is ResumeDoctor.AI, a resume‚ÄìJD analysis system. It extracts keywords from resumes, compares them with job descriptions, and provides a match score along with improvement suggestions. I deployed this project on Render and handled the complete backend implementation.

My second project is an AI-Based Learning-to-Content Automation System, where I automated the process of converting long notes into short, ready-to-publish videos. I used Python along with external APIs for script generation, voice generation, image creation, and automated video assembly.

I also completed a Python backend internship at Labmentix, where I worked on API development, SQL tasks, debugging backend issues, and understanding real development workflows, which strengthened my practical problem-solving abilities.

Since my strongest foundation is in Python and databases, I chose the Python + Database + PySpark domain because it aligns naturally with my existing skills. I am currently building my PySpark fundamentals ‚Äî especially working with DataFrames, transformations, joins, and simple ETL concepts ‚Äî so I can contribute effectively to data-driven and scalable backend systems as well.

resume->
Skills

Languages: Python
Frameworks: Flask
Databases: Oracle SQL

Database Skills:

Good understanding of RDBMS concepts (constraints, keys, normalization, tables)

Excellent knowledge of writing SQL queries

Strong understanding of Grouping, Subqueries, SQL Functions

Solid understanding of SQL Joins (Inner, Left, Right, and Full Joins)

Good knowledge of DDL, DML, and TCL operations

Backend Skills:

REST API development using Flask

Designing clean and scalable backend logic

Integrating databases with backend APIs

Other Skills:

Data Structures & Algorithms (DSA)

Git & Version Control

Internship Experience
Python Backend Developer Intern ‚Äî Labmentix.in

Apr 2025 ‚Äì Oct 2025

Developed backend APIs using Flask with proper routing, validations, and structured responses.

Worked extensively with Oracle SQL for table design, writing optimized queries, CRUD operations, joins, and indexing.

Integrated database operations within backend APIs and improved query performance.

Debugged backend modules, optimized API flow, and ensured stable backend‚Äìdatabase connectivity.

Participated in Agile sprints and contributed to backend modules with strong problem-solving and debugging skills.

Projects
ResumeDoctor.AI ‚Äî Resume-JD Matcher (Backend Project)

Built the backend using Flask and integrated with Oracle SQL to store resumes, extracted data, and match results.

Implemented logic for keyword extraction, scoring, and comparison between resumes and job descriptions.

Designed and structured database tables for efficient storage and analysis.

Deployed the project on Render Cloud.

Created a basic frontend using HTML, CSS, Bootstrap, and minimal JavaScript only for testing and connecting APIs (backend was the main focus).

AI-Based Learning-to-Content Automation System (Python Backend Project)

Built a complete backend workflow in Python to convert long educational notes into short, ready-to-publish videos.

Used external APIs for script generation, text-to-speech, image generation, and automated video creation.

Managed complete backend execution including workflow orchestration, data processing, and error handling.

Implemented a simple UI using HTML/CSS/JS for triggering backend tasks, while the major work remained backend-centric.

Certifications

Python Programming Fundamentals

Issuing Organization: Microsoft

Date Completed: October 4, 2025

Credential Link: Available on request

Web Development with Python

Issuing Organization: Microsoft

Date Completed: October 9, 2025

Credential Link: Available on request

Learning / Familiar With:

PySpark (DataFrames, basic transformations, joins)


‚ÄúWhy didn‚Äôt you put PySpark in main skills?‚Äù

Use this perfect answer:

‚ÄúI am currently building my PySpark fundamentals‚Äîmainly DataFrames, basic transformations, joins, and simple ETL logic.
I didn‚Äôt want to list it as a core skill until I complete a hands-on project.
But I‚Äôm comfortable with beginner-level concepts and actively improving.‚Äù

Safer, Shorter Version (use if they ask quickly):

‚ÄúI built the full frontend myself using HTML, CSS, Bootstrap, and basic JS.
I used AI tools only to speed up template creation and get UI suggestions, but all the integration and customization was done by me.‚Äù




---

# ‚úÖ **üî• PERFECT ANSWER (Use this in interview)**

**‚ÄúI selected the Python, Database, and PySpark domain because my strongest foundation is already in Python backend development using Flask and SQL.
Adding PySpark on top of this aligns naturally with what I already know.
Instead of shifting to completely different technologies like Java or React, I prefer to go deeper in Python-based backend and data engineering fundamentals.
This domain allows me to use my existing Python + SQL skills while learning PySpark for scalable data processing, which is highly relevant for modern backend systems.
So this choice gives me both confidence and growth.‚Äù**

**This is the BEST answer: it shows logic + honesty + future potential.**

---

---

# üü© **Short Answer (HR Friendly)**

**‚ÄúI am strongest in Python and databases, and I want to build on existing skills. PySpark fits naturally with Python, so this domain aligns with my expertise and future career goals.‚Äù**

---

# üüß **Detailed Answer (Technical Panel)**

**‚ÄúI already have solid hands-on experience in Python backend development using Flask and SQL databases. When I saw the domain options, Python + Database + PySpark was the most logical extension of my current skillset.
PySpark is widely used in scalable backend applications, ETL pipelines, and data processing systems, so learning it will help me become a more well-rounded backend developer.
Instead of picking domains like Java/React that require starting from scratch, I chose the domain where I can contribute quickly and grow efficiently.‚Äù**

---

# üü• **Strongest Answer (Face-to-face Round)**

Use this for *maximum impact*:

**‚ÄúMy goal is to become a strong backend engineer.
I already built projects using Flask, Python, and SQL, so selecting this domain helps me strengthen the backend track I have started.
PySpark is a great addition because modern backend systems handle large data, and PySpark is essential for distributed data pipelines.
This domain keeps my learning path aligned ‚Äî Python backend + database + scalable data processing.
It feels natural and consistent rather than shifting to unrelated stacks like Java or React.‚Äù**

Modern companies‚Äîincluding Adobe‚Äîwork heavily with data-driven and scalable systems.
Python gives me the flexibility to build backend logic, databases give me the foundation to structure and manage data correctly, and PySpark allows me to process large datasets efficiently.

This domain connects all three skills in a very natural way. It matches my existing strengths, aligns with my learning path, and supports the kind of career I want to grow into: a backend engineer who is also comfortable with data pipelines and distributed processing.‚Äù**

---

# üéØ **Now Let's Prepare for Possible Follow-Up Questions**

### **1. ‚ÄúBut you don‚Äôt have PySpark experience. Why choose it?‚Äù**

Answer:
**‚ÄúYes, I don‚Äôt have a PySpark project yet, but PySpark is built on Python, and its DataFrame syntax is similar to Pandas. I can pick it up quickly, and I have already started learning PySpark transformations, joins, and basic ETL tasks.‚Äù**

---

### **2. ‚ÄúWhy didn‚Äôt you choose Java/React domain instead?‚Äù**

Answer:
**‚ÄúThose domains require starting from zero.
Choosing the Python + DB + PySpark domain allows me to build on what I already know and become productive faster.‚Äù**

---

### **3. ‚ÄúAre you comfortable learning PySpark on the job?‚Äù**

Answer:
**‚ÄúAbsolutely. I have already started practicing DataFrames, joins, aggregations, and SQL queries in PySpark.
Since it extends Python, I can learn it quickly and contribute effectively.‚Äù**

---

### **4. ‚ÄúDo you want to be a backend developer or data engineer?‚Äù**

Safe answer:
**‚ÄúMy primary target is backend development, but I want to be flexible with data engineering tasks too. This domain gives me the chance to handle both API-level and data-level responsibilities.‚Äù**

---

# üü® **Extra Tip: Mention your Flask + DB Projects Smartly**

When they ask:

> ‚ÄúWhy not choose Java/React domain?‚Äù

You say:
**‚ÄúBecause I have already built real backend systems in Python + Flask + SQL.
This domain allows me to keep improving instead of resetting my learning path.‚Äù**

---

# üü¶ **Final Ready-Made Answer (Use this EXACT intro in interview)**

**‚ÄúI chose Python + Database + PySpark because I already have a strong foundation in Python backend using Flask and SQL.
This domain allows me to build on my strengths while learning PySpark for scalable data processing.
Instead of switching to completely different areas like Java or React, I wanted to stay aligned with backend technologies where I can contribute quickly and grow effectively.‚Äù**

---
Got it.
You are **Python + Database strong**, but **PySpark = ZERO knowledge**, and your interview domain includes PySpark.

So the interviewer will expect:

* **Basic** PySpark
* **Not deep big data engineering**
* **No real project**, just basic understanding
* **One small 10‚Äì15 line PySpark task**
* **Concept-level questions**

You only need ‚Üí **Beginner + Interview-level PySpark**, not real big-data engineering.

Don‚Äôt worry ‚Äî I‚Äôll give you a **crash course + all expected questions + safe answers** for someone with *no PySpark project*.

---

# üéØ **HOW TO ANSWER ‚ÄúDo you have PySpark project experience?‚Äù**

You should say:

> ‚ÄúI used PySpark at a learning level to understand DataFrames, transformations, joins, and simple ETL operations.
> I did not build a full project in PySpark, but I am comfortable with beginner-to-intermediate level operations.‚Äù

This is **safe and honest**, and interviewers accept this for apprenticeships.

Now, I will teach you ONLY what you need to CLEAR the interview.

---

# ‚úÖ **PYSPARK CRASH COURSE (ZERO ‚Üí INTERVIEW LEVEL)**

Simple, clean, and enough for your apprenticeship round.

---

# 1Ô∏è‚É£ What is PySpark?

### ‚≠ê Interview definition:

PySpark is the **Python API for Apache Spark**, used for processing **big datasets** in a distributed way.

### ‚≠ê Simple meaning:

Like Pandas, but works on **big data** and **multiple computers**.

---

# 2Ô∏è‚É£ SparkSession (Must Memorize)

```python
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("demo").getOrCreate()
```

This is how every PySpark program starts.

---

# 3Ô∏è‚É£ DataFrames (Your main focus)

### ‚≠ê Interview definition:

A PySpark DataFrame is a **distributed table** similar to Pandas DataFrame, but optimized for big data.

### Basic commands:

```python
df = spark.read.csv("data.csv", header=True, inferSchema=True)
df.show()
df.printSchema()
df.select("name", "age").show()
df.filter(df.age > 25).show()
```

---

# 4Ô∏è‚É£ Transformations & Actions

### ‚≠ê Transformations (lazy)

* `select()`
* `filter()`
* `withColumn()`
* `groupBy()`

### ‚≠ê Actions (execute jobs)

* `show()`
* `collect()`
* `count()`

### Interview Q:

**Q: Is PySpark lazy or eager?**
A: Transformations are **lazy**, and actions trigger execution.

---

# 5Ô∏è‚É£ Simple PySpark Operations (INTERVIEW TASKS)

### 5.1 Filter

```python
df.filter(df.salary > 50000).show()
```

### 5.2 Select

```python
df.select("name", "age").show()
```

### 5.3 Add New Column

```python
df.withColumn("bonus", df.salary * 0.1).show()
```

### 5.4 Group By

```python
df.groupBy("department").agg({"salary":"avg"}).show()
```

---

# 6Ô∏è‚É£ Joins (VERY IMPORTANT)

Joins they expect you to know:

* inner
* left
* right
* full

### Example:

```python
df1.join(df2, "id", "inner")
```

### Expected interview Q:

**Q: How do you perform join in PySpark?**
**Q: What is left join?**

---

# 7Ô∏è‚É£ Handling Missing Data

```python
df.dropna()
df.fillna({"age": 0})
```

---

# 8Ô∏è‚É£ Read File Formats (Very high chance)

### Interview answers:

* CSV ‚Üí slow
* JSON ‚Üí hierarchical
* Parquet ‚Üí **best for big data**
* ORC ‚Üí optimized for Hadoop

---

# 9Ô∏è‚É£ Basic SQL in PySpark

```python
df.createOrReplaceTempView("emp")
spark.sql("SELECT * FROM emp WHERE age > 30").show()
```

### Interview Q:

**Q: How to run SQL queries in PySpark?**

---

# üî• THAT'S ALL YOU NEED TECHNICALLY

This is enough beginner‚Äìintermediate knowledge for apprenticeship.

---

# üü© **SAFE ANSWERS for ‚ÄúNo PySpark project‚Äù Situation**

### When they ask:

> ‚ÄúHave you done a project with PySpark?‚Äù

You say:

> ‚ÄúI don‚Äôt have an end-to-end PySpark project yet,
> but I **used PySpark to practice** DataFrames, joins, aggregations, and simple ETL tasks.
> My main hands-on projects are in **Flask + Python + SQL**,
> and I am actively learning PySpark fundamentals.‚Äù

This is **honest + safe + acceptable**.

---

# üü¶ **EXPECTED PYSPARK INTERVIEW QUESTIONS (WITH PERFECT ANSWERS)**

Copied exactly as companies ask.

---

### **Q1: What is PySpark?**

A: PySpark is the Python API for Apache Spark, used for distributed data processing.

---

### **Q2: Difference between Pandas and PySpark?**

* Pandas ‚Üí small data, single machine
* PySpark ‚Üí big data, distributed processing

---

### **Q3: What is SparkSession?**

It is the entry point to PySpark programs.

---

### **Q4: What is lazy evaluation?**

Transformations are lazy, actions execute the pipeline.

---

### **Q5: What are transformations? Give examples.**

`select()`, `filter()`, `groupBy()`, `withColumn()`.

---

### **Q6: What are actions?**

`show()`, `collect()`, `count()`.

---

### **Q7: How do you read a CSV in PySpark?**

```python
df = spark.read.csv("file.csv", header=True, inferSchema=True)
```

---

### **Q8: How do you handle NULL values?**

`fillna()`, `dropna()`.

---

### **Q9: Explain joins in PySpark.**

Inner, left, right, full join.

```python
df1.join(df2, "id", "inner")
```

---

### **Q10: What is Parquet file? Why preferred?**

Columnar, compressed, fastest for big data.

---

# üü® **INTERVIEW TASK THEY MAY GIVE YOU**

You MUST practice this.

**Task:**
A DataFrame: id, name, age, salary

* Filter employees age > 30
* Add a bonus = 10% salary
* Group by age
* Show average salary

Solution:

```python
df2 = df.filter(df.age > 30)
df3 = df2.withColumn("bonus", df.salary * 0.1)
df3.groupBy("age").avg("salary").show()
```

---

# üéØ **YOU ARE NOW INTERVIEW-READY FOR BASIC‚ÄìINTERMEDIATE Pyspark**

You learned:

* What PySpark is
* DataFrames
* Transformations
* Actions
* Joins
* Null handling
* GroupBy
* SQL
* File formats
* Beginner ETL tasks
* Safe answers regarding no project

---

# ‚úî If you want, I can now provide:

### üü¢ PySpark **Cheat Sheet (1 page)**

### üü¢ PySpark **20 coding tasks with solutions**

### üü¢ PySpark **Practice DataFrame questions**

### üü¢ PySpark **mock interview**

### üü¢ A small **PySpark mini-project** (easy + interview-friendly)

Tell me what you want next.
Below is the **most complete and realistic list of questions** an interviewer will ask if you say:

> ‚ÄúI am a Python Backend Developer using Flask and I built 2 projects.‚Äù

These questions are arranged from **basic ‚Üí intermediate ‚Üí advanced**, including:

* Python fundamentals
* Flask core concepts
* Project-based questions (deep authenticity checks)
* Database + APIs + real backend scenarios
* Debugging, architecture, and deployment
* Trick questions asked by companies like **Adobe, Google, Amazon, PayPal, Zeta, and startups**

This is EXACTLY what you need for a face-to-face interview.

---

# ‚úÖ **SECTION 1 ‚Äî PYTHON BASICS (This will surely be asked)**

They ask to check whether you know Python fundamentals or you only copied Flask code.

### **1. Core Data Types**

* What are mutable vs immutable types?
* Explain list, tuple, set, dict differences.
* What is the output of:

  ```python
  a = [1,2,3]
  b = a
  a.append(4)
  print(b)
  ```

---

### **2. Functions**

* What are *args and **kwargs?
* What is lambda function?
* What is the difference between return and yield?
* What is recursion? Give real use case.

---

### **3. OOP (Mandatory)**

* What is a class? What is an object?
* What is `__init__`?
* Difference: `__str__` vs `__repr__`.
* Explain inheritance, composition, abstraction.
* What is MRO?
* What is super() and when to use?

---

### **4. Important Python Concepts**

* What is a generator?
* What is an iterator?
* Difference between shallow copy and deep copy?
* What is GIL (Global Interpreter Lock)?
* What is multithreading vs multiprocessing?

---

### ‚≠ê EXPECT practical code questions like:

> Write a Python function to remove duplicates without using set.
> Write a generator that yields even numbers.
> Write a decorator for logging.

---

# ‚úÖ **SECTION 2 ‚Äî FLASK CORE CONCEPTS (MOST IMPORTANT)**

### **1. What is Flask? Why used over Django?**

They check your reasoning.

---

### **2. Routing**

* How routes are created?
* What are route parameters?
* Difference between GET, POST, PUT, DELETE?
* What is url_for()?

---

### **3. Request & Response**

* `request.form`, `request.args`, `request.json` differences
* What is jsonify()?
* What is response status code?
* How to return custom headers?

---

### **4. Templates (Jinja2)**

* What is Jinja2?
* How do you pass data to templates?
* What are filters in Jinja?

---

### **5. Flask Application Structure**

* What is the purpose of app.py?
* What is **init**.py in Flask?
* Why use blueprints?
* What is the difference between app.run() and WSGI server?

---

### **6. Flask Configurations**

* What are environment variables?
* How do you manage dev/test/prod configs?

---

### **7. Flask Extensions**

They will ask if your project used:

* Flask-SQLAlchemy
* Flask-Migrate
* Flask-Login
* Flask-RESTful
* Flask-CORS
* Flask-JWT-Extended

---

### **8. Authentication & Authorization**

* Explain JWT in depth.
* What is access token vs refresh token?
* How do you store passwords?

---

### **9. Error Handling**

* What is the use of `@app.errorhandler`?
* How to return custom error JSON?

---

# ‚≠ê KEY CHECKPOINT:

They will check authenticity by asking:
**‚ÄúWhat problem did you face in Flask and how did you fix it?‚Äù**

---

# ‚úÖ **SECTION 3 ‚Äî DATABASE & ORM (Compulsory for Backend)**

### **1. SQL Queries**

* JOIN types: INNER, LEFT, RIGHT
* GROUP BY & HAVING
* What is indexing?
* ACID properties

---

### **2. SQLAlchemy (Most Asked)**

* What is ORM?
* What is session in SQLAlchemy?
* Lazy loading vs eager loading
* How to avoid N+1 query problem?

---

### **3. Migrations**

* What is Alembic?
* How to migrate schema?

---

# ‚≠ê AUTHENTICITY CHECK QUESTION:

> Show me the schema of your project database.
> Explain each table and relationship.

---

# ‚úÖ **SECTION 4 ‚Äî API DEVELOPMENT (Very Important)**

### **1. What is REST API?**

* REST vs SOAP
* REST constraints
* Idempotency: PUT vs POST

---

### **2. API Response Structure**

* Return structure
* Status codes
* Error handling

---

### **3. Pagination**

* How do you handle pagination in APIs?

---

### **4. Rate Limiting**

* What is throttling?
* How to implement rate limiting in Flask?

---

### **5. Versioning**

* API versioning best practices?

---

# ‚≠ê DEEP CHECK:

> How do you secure your API?

Expect:

* JWT
* Role-based auth
* CORS
* CSRF

---

# ‚úÖ **SECTION 5 ‚Äî REAL PROJECT DEEP INTERVIEW (MOST IMPORTANT)**

They will ask deep questions about your 2 Flask projects to check if YOU built them.

Here‚Äôs EXACTLY what they‚Äôll ask:

---

## **1. Explain your project architecture.**

* How many modules?
* Folder structure?

---

## **2. Tell the major functional flows.**

Example: User Registration ‚Üí Login ‚Üí JWT authentication ‚Üí Dashboard

---

## **3. Database design**

They will ask:

* Show me your tables.
* Why this column?
* Why this relationship?
* Why this index?

---

## **4. Real Problems you faced**

This reveals authenticity.

Examples:

* CORS errors
* Slow queries
* SQLAlchemy session issues
* Circular imports
* JWT expiration issues
* Deployment issues

---

## **5. Scaling Questions**

They will check whether you know real backend concepts.

Questions:

* How would your Flask project scale if traffic increases?
* What is caching?
* What is load balancing?
* How to make Flask thread-safe?
* How to handle concurrency?

---

## **6. Deployment (Very Important)**

If you deployed:

They will ask:

* Where did you deploy?
* How did you deploy Flask?
* What is WSGI?
* Difference between Gunicorn and uWSGI?
* Reverse proxy: Why Nginx?

---

# ‚≠ê **MOST IMPORTANT AUTHENTICITY CHECK (THEY WILL DEFINITELY ASK THIS)**

> ‚ÄúCan you write a small Flask API here in the interview?‚Äù

Example request:

* Create a POST API for adding a user
* Validate input
* Save to DB
* Return success JSON

OR

> ‚ÄúWrite an API that returns a list of students sorted by age.‚Äù

---

# ‚ùó **If you fail this, they know you copied the project.**

---

# üî• **SECTION 6 ‚Äî ADVANCED QUESTIONS FOR ADOBE-LEVEL**

### **1. What is WSGI and why Flask needs it?**

Answer:
Flask is not a production server. WSGI connects Flask to servers like Gunicorn.

---

### **2. How to solve circular import issue in Flask?**

Answer:
Using **Blueprints**, or moving imports inside functions.

---

### **3. What is Flask's application context vs request context?**

* `g`, `current_app` ‚Üí application context
* `request`, `session` ‚Üí request context

---

### **4. What is thread-safety in Flask?**

Answer:
Flask uses a global object `request` which is thread-local, not shared across threads.

---

### **5. How do you handle file uploads in Flask?**

Using:

* `request.files`
* secure_filename()

---

### **6. What is a race condition?**

Example: two requests updating same record at the same time.

---

### **7. What is DB connection pooling?**

Used to reuse DB connections ‚Üí improves performance.

---

### **8. Difference between synchronous and asynchronous Flask?**

Flask is not async by default.
FastAPI supports async.

---

# üéØ **FINAL: Python + Flask + DB + Auth Deep Questions for Authenticity**

(You MUST be prepared)

1. Write a login API using Flask.
2. Hash a password using bcrypt.
3. Write a JOIN query in SQLAlchemy.
4. Explain JWT complete flow.
5. How do you store refresh tokens?
6. How do you block a stolen token?
7. How to log every request?
8. How do you optimize a slow API?
9. How do you prevent SQL injection?
10. How to test APIs using Postman?
11. How do you write unit tests in Flask?
12. What is app context push?
13. What happens when two users call same API simultaneously?
14. How do you handle 1 million API calls?
15. How to store session in Redis?
16. How to monitor Flask logs in production?

---

# If you want, I can now give you:

### ‚úî Complete **Flask backend cheat sheet (1 page)**

### ‚úî Your **2 projects authenticity preparation questions**

### ‚úî A **mock technical interview (Python + Flask)**

### ‚úî Full code templates for Flask interview

Just tell me **which one you want next**.
 Section 18 - Interview Closing Questions
Perfect Questions to Ask Interviewer
"What does a typical day look like for someone in this role?"

Shows genuine interest in day-to-day work
"What are the key technologies and tools the team uses?"

Shows technical curiosity
"What opportunities are there for learning and growth?"

Shows long-term thinking
"What do you enjoy most about working here?"

Personal connection, positive framing
"Is there any advice you would give someone entering the Apprentice role to perform well early?"

Shows eagerness to succeed
How to Ask for Feedback
Approach: "I'm always looking to improve. Is there any feedback you can share about my interview today?"

If They Say "We cannot give feedback"
Response: "I understand. Thank you for your time today. I really enjoyed learning about the role and the team. I'm excited about the opportunity and look forward to hearing from you."
